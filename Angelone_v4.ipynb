{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Tanmay3463/SWEET_algo_trading_software/blob/main/Angelone_v4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/angel-one/smartapi-python.git\n",
        "!pip install pyotp logzero websocket-client pycryptodome python-dotenv requests"
      ],
      "metadata": {
        "id": "9yTFVANrXiCA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0d944de0-310e-4d98-9823-96a6c1fd5407"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'smartapi-python' already exists and is not an empty directory.\n",
            "Requirement already satisfied: pyotp in /usr/local/lib/python3.12/dist-packages (2.9.0)\n",
            "Requirement already satisfied: logzero in /usr/local/lib/python3.12/dist-packages (1.7.0)\n",
            "Requirement already satisfied: websocket-client in /usr/local/lib/python3.12/dist-packages (1.8.0)\n",
            "Requirement already satisfied: pycryptodome in /usr/local/lib/python3.12/dist-packages (3.23.0)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.12/dist-packages (1.1.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (2.32.4)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests) (2025.8.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Imports**"
      ],
      "metadata": {
        "id": "JDTS5k9eRTk1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.append('/content/smartapi-python')\n",
        "\n",
        "import os\n",
        "import pyotp\n",
        "import pandas as pd\n",
        "import requests\n",
        "from dotenv import load_dotenv\n",
        "from logzero import logger\n",
        "from SmartApi import SmartConnect\n",
        "from datetime import datetime, timedelta\n",
        "import gradio as gr\n",
        "import time\n",
        "import json\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import logging\n",
        "import math\n",
        "import shutil\n",
        "\n",
        "# --- Global variables for separate exchange maps ---\n",
        "nse_symbol_token_map = {}\n",
        "bse_symbol_token_map = {}\n",
        "nse_symbols_list = []\n",
        "bse_symbols_list = []\n",
        "\n",
        "_industry_df_cache = None"
      ],
      "metadata": {
        "id": "GvrXBs3iYSAy"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "directory_path = \"/content/downloadrohit/backup/\"\n",
        "os.makedirs(directory_path, exist_ok=True)\n",
        "\n",
        "directory_path2 = \"/content/masterrohit/\"\n",
        "os.makedirs(directory_path2, exist_ok=True)"
      ],
      "metadata": {
        "id": "FqHyceli-ZOU"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Login**"
      ],
      "metadata": {
        "id": "Az_4Ww7QRgGe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# --- Setup Logging ---\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "\n",
        "# --- Load .env and Initialize API (User's Code) ---\n",
        "load_dotenv()\n",
        "api_key = os.getenv(\"API_KEY_TRADING\")\n",
        "username = os.getenv(\"USERNAME\")\n",
        "pwd = os.getenv(\"PASSWORD\")\n",
        "totp_token = os.getenv(\"TOTP_TOKEN\")\n",
        "\n",
        "smartApi = None\n",
        "login_status_message = \"\"\n",
        "\n",
        "if not all([api_key, username, pwd, totp_token]):\n",
        "    login_status_message = \"â—ï¸ Missing API_KEY, USERNAME, PASSWORD, or TOTP_TOKEN in .env file\"\n",
        "    logging.error(login_status_message)\n",
        "else:\n",
        "    smartApi = SmartConnect(api_key)\n",
        "    try:\n",
        "        totp = pyotp.TOTP(totp_token).now()\n",
        "        session_data = smartApi.generateSession(username, pwd, totp)\n",
        "        if session_data and session_data.get('status'):\n",
        "             login_status_message = \"âœ… Login successful!\"\n",
        "             logging.info(login_status_message)\n",
        "        else:\n",
        "            login_status_message = f\"âŒ Login failed: {session_data.get('message', 'Unknown error')}\"\n",
        "            logging.error(login_status_message)\n",
        "            smartApi = None\n",
        "    except Exception as e:\n",
        "        login_status_message = f\"âŒ Login failed with an exception: {e}\"\n",
        "        logging.error(login_status_message)\n",
        "        smartApi = None"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LDKfLv47ZWN_",
        "outputId": "449e1cdc-d41c-4731-b79a-72c549ddc65f"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 250927 07:11:06 smartConnect:124] in pool\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "api_key_history = os.getenv(\"API_KEY_HISTORY\")\n",
        "username = os.getenv(\"USERNAME\")\n",
        "pwd = os.getenv(\"PASSWORD\")\n",
        "totp_token = os.getenv(\"TOTP_TOKEN\")\n",
        "\n",
        "if not all([api_key_history, username, pwd, totp_token]):\n",
        "    raise ValueError(\"Missing API_KEY, USERNAME, PASSWORD, or TOTP_TOKEN in .env file\")\n",
        "\n",
        "smartApi_hist = SmartConnect(api_key_history)\n",
        "\n",
        "try:\n",
        "    totp_hist = pyotp.TOTP(totp_token).now()\n",
        "except Exception as e:\n",
        "    logger.error(\"Invalid TOTP Token\")\n",
        "    raise e\n",
        "\n",
        "try:\n",
        "    session_data_hist = smartApi_hist.generateSession(username, pwd, totp)\n",
        "    logger.info(\"âœ… Login successful!\")\n",
        "except Exception as e:\n",
        "    logger.error(f\"âŒ Login failed: {e}\")\n",
        "\n",
        "print(totp)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5HpCu7GKa8s_",
        "outputId": "6eeedbb0-4fe1-412b-c685-c8515d12c6e7"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 250927 07:11:07 smartConnect:124] in pool\n",
            "[I 250927 07:11:09 ipython-input-1567320128:19] âœ… Login successful!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "739544\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nse = \"downloadrohit/script_master_NSE.csv\"\n",
        "bse = \"downloadrohit/script_master_BSE.csv\"\n",
        "portfolio = \"downloadrohit/portfolio_holdings.csv\"\n",
        "order_book = \"downloadrohit/order_book.csv\"\n",
        "trade_book = \"downloadrohit/trade_book.csv\"\n",
        "rule_book = 'masterrohit/Rule_book.csv'\n",
        "industry = \"masterrohit/Dashboard_Industry.csv\"\n",
        "master_portfolio = 'masterrohit/master_portfolio.csv'\n",
        "archive = 'masterrohit/archive.csv'\n",
        "SWEET_Code_path = 'masterrohit/SWEET_Code.csv'\n",
        "SLEEP = 0.1"
      ],
      "metadata": {
        "id": "8vv8q_jIlrKk"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "\n",
        "# Constants for the URL and local file paths\n",
        "JSON_URL = \"https://margincalculator.angelbroking.com/OpenAPI_File/files/OpenAPIScripMaster.json\"\n",
        "NSE_CSV_PATH = nse\n",
        "BSE_CSV_PATH = bse\n",
        "\n",
        "\n",
        "def _fetch_from_url(url: str) -> list | None:\n",
        "    \"\"\"\n",
        "    Attempts to download and parse the instrument master JSON from the given URL.\n",
        "    Returns the JSON data as a list of dictionaries, or None if it fails.\n",
        "    \"\"\"\n",
        "    logging.info(f\"Attempting to download script master from {url}\")\n",
        "    try:\n",
        "        # Added a timeout for robustness\n",
        "        response = requests.get(url, timeout=10)\n",
        "        response.raise_for_status()  # Raises an HTTPError for bad responses (4xx or 5xx)\n",
        "        return response.json()\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        logging.error(f\"Error during download: {e}\")\n",
        "        return None\n",
        "    except ValueError as e:\n",
        "        logging.error(f\"Error decoding JSON: {e}\")\n",
        "        return None\n",
        "\n",
        "def _process_and_save_data(script_data: list) -> tuple[list, list, dict]:\n",
        "    \"\"\"\n",
        "    Processes the raw script data, saves it to separate CSVs for NSE and BSE,\n",
        "    and returns the symbols lists and token map.\n",
        "    \"\"\"\n",
        "    if not script_data:\n",
        "        logging.warning(\"Received empty script data. Cannot process.\")\n",
        "        return [], [], {}\n",
        "\n",
        "    df = pd.DataFrame(script_data)\n",
        "\n",
        "    # Filter data for NSE and BSE\n",
        "    nse_df = df[df['exch_seg'] == 'NSE'].copy()\n",
        "    bse_df = df[df['exch_seg'] == 'BSE'].copy()\n",
        "\n",
        "    # Save to CSV files\n",
        "    nse_df.to_csv(NSE_CSV_PATH, index=False)\n",
        "    logging.info(f\"Successfully saved {len(nse_df)} NSE instruments to {NSE_CSV_PATH}\")\n",
        "    bse_df.to_csv(BSE_CSV_PATH, index=False)\n",
        "    logging.info(f\"Successfully saved {len(bse_df)} BSE instruments to {BSE_CSV_PATH}\")\n",
        "\n",
        "    # Create the required data structures\n",
        "    nse_symbols = sorted(nse_df['symbol'].dropna().unique().tolist())\n",
        "    bse_symbols = sorted(bse_df['symbol'].dropna().unique().tolist())\n",
        "\n",
        "    # Create mapping with exchange key to avoid clashes\n",
        "    # Drop rows where key components are missing to prevent errors\n",
        "    token_df = df.dropna(subset=['exch_seg', 'symbol', 'token'])\n",
        "    token_map = {\n",
        "        (row.exch_seg, row.symbol): row.token\n",
        "        for row in token_df.itertuples()\n",
        "    }\n",
        "\n",
        "    return nse_symbols, bse_symbols, token_map\n",
        "\n",
        "def _load_from_csv() -> tuple[list, list, dict]:\n",
        "    \"\"\"\n",
        "    Loads instrument data from local CSV files as a fallback.\n",
        "    Returns the symbols lists and token map.\n",
        "    \"\"\"\n",
        "    if not os.path.exists(NSE_CSV_PATH) or not os.path.exists(BSE_CSV_PATH):\n",
        "        logging.error(\"Fallback CSV files not found. Cannot load data.\")\n",
        "        return [], [], {}\n",
        "\n",
        "    try:\n",
        "        logging.info(f\"Loading data from local files: {NSE_CSV_PATH} and {BSE_CSV_PATH}\")\n",
        "        nse_df = pd.read_csv(NSE_CSV_PATH)\n",
        "        bse_df = pd.read_csv(BSE_CSV_PATH)\n",
        "\n",
        "        # Combine for the token map\n",
        "        combined_df = pd.concat([nse_df, bse_df])\n",
        "\n",
        "        nse_symbols = sorted(nse_df['symbol'].dropna().unique().tolist())\n",
        "        bse_symbols = sorted(bse_df['symbol'].dropna().unique().tolist())\n",
        "\n",
        "        token_df = combined_df.dropna(subset=['exch_seg', 'symbol', 'token'])\n",
        "        token_map = {\n",
        "            (row.exch_seg, row.symbol): row.token\n",
        "            for row in token_df.itertuples()\n",
        "        }\n",
        "\n",
        "        return nse_symbols, bse_symbols, token_map\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Failed to load or process data from CSV files: {e}\")\n",
        "        return [], [], {}\n",
        "\n",
        "def load_instrument_master() -> tuple[list, list, dict]:\n",
        "\n",
        "    script_data = _fetch_from_url(JSON_URL)\n",
        "\n",
        "    if script_data:\n",
        "        # If download was successful, process and save the new data\n",
        "        return _process_and_save_data(script_data)\n",
        "    else:\n",
        "        # If download failed, trigger the fallback\n",
        "        logging.warning(\"Download failed. Attempting to load from local cache.\")\n",
        "        return _load_from_csv()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # This is how you would use the function in your main script\n",
        "    nse_symbols, bse_symbols, token_map = load_instrument_master()\n",
        "\n",
        "    if token_map:\n",
        "        logging.info(f\"Loaded {len(nse_symbols)} NSE and {len(bse_symbols)} BSE symbols.\")\n",
        "\n",
        "        # Example: Look up the token for a specific stock\n",
        "        reliance_token = token_map.get(('NSE', 'RELIANCE-EQ'))\n",
        "        if reliance_token:\n",
        "            logging.info(f\"The token for RELIANCE-EQ on NSE is: {reliance_token}\")\n",
        "        else:\n",
        "            logging.warning(\"Could not find token for RELIANCE-EQ on NSE.\")\n",
        "\n",
        "        # Example: Check the first 5 symbols in the NSE list\n",
        "        logging.info(f\"First 5 NSE symbols: {nse_symbols[:5]}\")\n",
        "    else:\n",
        "        logging.error(\"Failed to load instrument master from all sources.\")\n"
      ],
      "metadata": {
        "id": "TGFjhVSdZlpN"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Default date range\n",
        "today = datetime.now()\n",
        "fifty_two_weeks_ago = today - timedelta(weeks=52)\n",
        "default_from_date = fifty_two_weeks_ago.strftime(\"%Y-%m-%d %H:%M\")\n",
        "default_to_date = today.strftime(\"%Y-%m-%d %H:%M\")"
      ],
      "metadata": {
        "id": "-NIUZBjDZvC3"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Historical Data**"
      ],
      "metadata": {
        "id": "ONV6uu-KRnRY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def update_symbol_dropdown(exchange):\n",
        "    return gr.Dropdown(choices=nse_symbols if exchange == \"NSE\" else bse_symbols, multiselect=True, interactive=True)\n",
        "\n",
        "def select_all_symbols(exchange):\n",
        "    return nse_symbols if exchange == \"NSE\" else bse_symbols"
      ],
      "metadata": {
        "id": "YGpiyO8rZ3hB"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_historical_data_for_tokens(symbol_tokens, exchange, interval, fromdate, todate):\n",
        "    all_candle_data = {}\n",
        "    # Create a mapping from token back to symbol for easier lookup\n",
        "    token_to_symbol = {token: symbol for (exch, symbol), token in token_map.items() if exch == exchange and token in symbol_tokens}\n",
        "\n",
        "    for token in symbol_tokens:\n",
        "        params = {\n",
        "            \"exchange\": exchange,\n",
        "            \"symboltoken\": token,\n",
        "            \"interval\": interval,\n",
        "            \"fromdate\": fromdate,\n",
        "            \"todate\": todate\n",
        "        }\n",
        "        logger.info(f\"ðŸ“¤ Requesting data with params: {params}\")\n",
        "\n",
        "        try:\n",
        "            candle_data = smartApi_hist.getCandleData(params)\n",
        "            logger.info(f\"ðŸ“¥ Response: {candle_data}\")\n",
        "\n",
        "            if candle_data and candle_data.get('status') and candle_data.get('data'):\n",
        "                df = pd.DataFrame(candle_data['data'], columns=['DateTime', 'Open', 'High', 'Low', 'Close', 'Volume'])\n",
        "                symbol = token_to_symbol.get(token, \"Unknown Symbol\")\n",
        "                df['Token Symbol'] = symbol\n",
        "                all_candle_data[token] = df\n",
        "            else:\n",
        "                logger.warning(f\"âš ï¸ No data for token: {token}\")\n",
        "        except Exception as e:\n",
        "            logger.exception(f\"âŒ Failed for token {token}: {e}\")\n",
        "        time.sleep(SLEEP*2)\n",
        "    return all_candle_data"
      ],
      "metadata": {
        "id": "kFvjLHR2Z-e0"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_data_and_prepare_download(input_type, selected_symbols, uploaded_file, exchange, interval, fromdate, todate):\n",
        "    if input_type == \"Manual Selection\":\n",
        "        if not selected_symbols:\n",
        "            return pd.DataFrame(), None, \"Please select at least one symbol.\"\n",
        "        symbols_to_process = selected_symbols\n",
        "    elif input_type == \"Upload Excel File\":\n",
        "        if not uploaded_file:\n",
        "            return pd.DataFrame(), None, \"Please upload an Excel file.\"\n",
        "        try:\n",
        "            df_upload = pd.read_excel(uploaded_file.name)\n",
        "            if 'Token Symbol' not in df_upload.columns:\n",
        "                return pd.DataFrame(), None, \"Uploaded file must contain a 'Token Symbol' column.\"\n",
        "            symbols_to_process = df_upload['Token Symbol'].tolist()\n",
        "        except Exception as e:\n",
        "            return pd.DataFrame(), None, f\"Error reading Excel file: {e}\"\n",
        "    else:\n",
        "        return pd.DataFrame(), None, \"Invalid input method selected.\"\n",
        "\n",
        "    # Process symbols to get unique list and remove any potential non-string values\n",
        "    symbols_to_process = [s for s in symbols_to_process if isinstance(s, str)]\n",
        "    symbols_to_process = list(set(symbols_to_process))\n",
        "\n",
        "    if not symbols_to_process:\n",
        "        return pd.DataFrame(), None, \"No valid symbols found in the selected or uploaded input.\"\n",
        "\n",
        "    try:\n",
        "        tokens = [token_map.get((exchange, s)) for s in symbols_to_process if (exchange, s) in token_map]\n",
        "        if not tokens:\n",
        "            return pd.DataFrame(), None, f\"Could not find tokens for selected symbols in {exchange}.\"\n",
        "\n",
        "        all_data = get_historical_data_for_tokens(tokens, exchange, interval, fromdate, todate)\n",
        "\n",
        "        if not all_data:\n",
        "            return pd.DataFrame(), None, \"No historical data retrieved for the selected symbols and date range.\"\n",
        "\n",
        "        combined_df = pd.concat(all_data.values(), ignore_index=True)\n",
        "\n",
        "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "        csv_filename = f\"downloadrohit/historical_data_{timestamp}.csv\"\n",
        "        combined_df.to_csv(csv_filename, index=False)\n",
        "\n",
        "        return combined_df, csv_filename, \"Data retrieved and ready for download.\"\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.exception(f\"An error occurred during data retrieval or processing: {e}\")\n",
        "        return pd.DataFrame(), None, f\"An error occurred: {e}\"\n"
      ],
      "metadata": {
        "id": "KMcM2EYoaHUV"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Master File**"
      ],
      "metadata": {
        "id": "MVsgGVlyRzYe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def download_script_master_csv(exchange: str) -> tuple[str | None, str]:\n",
        "    \"\"\"\n",
        "    Downloads and filters the script master data based on the selected exchange,\n",
        "    leveraging the efficient caching mechanism.\n",
        "\n",
        "    This function maintains the original signature for Gradio compatibility but\n",
        "    is powered by the robust load_instrument_master() function to avoid\n",
        "    redundant downloads. It saves the final output to a 'downloadrohit/' subdirectory.\n",
        "\n",
        "    Args:\n",
        "        exchange (str): The exchange to get the CSV for. Typically 'NSE' or 'BSE'.\n",
        "\n",
        "    Returns:\n",
        "        A tuple containing:\n",
        "        - file_path (str | None): The path to the CSV file, or None on failure.\n",
        "        - message (str): A status message.\n",
        "    \"\"\"\n",
        "    # First, ensure the data is loaded and caches are up-to-date.\n",
        "    nse_symbols, bse_symbols, token_map = load_instrument_master()\n",
        "\n",
        "    if not token_map:\n",
        "        return None, \"Failed to load instrument master data from all sources.\"\n",
        "\n",
        "    # Determine the correct source file path based on the exchange\n",
        "    if exchange == \"NSE\":\n",
        "        source_path = NSE_CSV_PATH\n",
        "    elif exchange == \"BSE\":\n",
        "        source_path = BSE_CSV_PATH\n",
        "    else:\n",
        "        message = f\"Invalid exchange '{exchange}'. Please use 'NSE' or 'BSE'.\"\n",
        "        logging.error(message)\n",
        "        return None, message\n",
        "\n",
        "    # Define the target directory and filename\n",
        "    target_dir = \"downloadrohit\"\n",
        "    target_path = os.path.join(target_dir, f\"script_master_{exchange}.csv\")\n",
        "\n",
        "    try:\n",
        "        # Ensure the target directory exists\n",
        "        os.makedirs(target_dir, exist_ok=True)\n",
        "\n",
        "        # Read the source cache file and save it to the target location\n",
        "        df = pd.read_csv(source_path)\n",
        "        df.to_csv(target_path, index=False)\n",
        "\n",
        "        message = f\"Script master for {exchange} successfully created at {target_path}\"\n",
        "        logging.info(message)\n",
        "        return target_path, message\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        message = f\"Cache file {source_path} not found. Cannot create derivative CSV.\"\n",
        "        logging.error(message)\n",
        "        return None, message\n",
        "    except Exception as e:\n",
        "        message = f\"An error occurred while creating {target_path}: {e}\"\n",
        "        logging.exception(message)\n",
        "        return None, message"
      ],
      "metadata": {
        "id": "Y7H9dIE21Wrw"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def toggle_input(choice):\n",
        "    if choice == \"Manual Selection\":\n",
        "        return gr.update(visible=True), gr.update(visible=True), gr.update(visible=False), gr.update(visible=True)\n",
        "    else:\n",
        "        return gr.update(visible=False), gr.update(visible=False), gr.update(visible=True), gr.update(visible=False)"
      ],
      "metadata": {
        "id": "sHAZxMF3aR8X"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Portfolio**"
      ],
      "metadata": {
        "id": "JjnLeWRsSAJi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_portfolio_holdings():\n",
        "    try:\n",
        "        portfolio_data = smartApi.holding()\n",
        "        if portfolio_data and portfolio_data.get('status'):\n",
        "            if 'data' in portfolio_data and portfolio_data['data']:\n",
        "                df = pd.DataFrame(portfolio_data['data'])\n",
        "                df = df.sort_values(by='tradingsymbol')\n",
        "\n",
        "                # Save CSV\n",
        "                csv_path = portfolio\n",
        "                df.to_csv(csv_path, index=False)\n",
        "\n",
        "                # Return records for DataFrame UI + CSV path\n",
        "                return df, csv_path\n",
        "            else:\n",
        "                df = pd.DataFrame([{\"Message\": \"No holdings found\"}])\n",
        "                return df, None\n",
        "        else:\n",
        "            df = pd.DataFrame([{\"Message\": portfolio_data.get('message', 'Unknown error')}])\n",
        "            return df, None\n",
        "    except Exception as e:\n",
        "        logger.error(f\"An error occurred while fetching portfolio data: {e}\")\n",
        "        return pd.DataFrame([{\"Error\": str(e)}]), None\n"
      ],
      "metadata": {
        "id": "JV8LuGtYX0Pe"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Order History**"
      ],
      "metadata": {
        "id": "bLTZx4a6SJoO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_script_masters():\n",
        "    \"\"\"Loads the NSE and BSE script master files from local CSVs.\"\"\"\n",
        "    global nse_symbol_token_map, bse_symbol_token_map\n",
        "\n",
        "    nse_symbol_token_map, bse_symbol_token_map = {}, {}  # Always initialize\n",
        "\n",
        "    nse_file = nse\n",
        "    bse_file = bse\n",
        "\n",
        "    # Load NSE master\n",
        "    if os.path.exists(nse_file):\n",
        "        logging.info(\"Loading NSE script master from local file...\")\n",
        "        nse_master_df = pd.read_csv(nse_file)\n",
        "        if not nse_master_df.empty:\n",
        "            nse_master_df['token'] = nse_master_df['token'].astype(str)\n",
        "            nse_symbol_token_map = pd.Series(\n",
        "                nse_master_df.token.values,\n",
        "                index=nse_master_df.symbol\n",
        "            ).to_dict()\n",
        "            logging.info(\"NSE Symbol-to-Token map created.\")\n",
        "    else:\n",
        "        logging.warning(f\"'{nse_file}' not found. NSE lookups will fail.\")\n",
        "\n",
        "    # Load BSE master\n",
        "    if os.path.exists(bse_file):\n",
        "        logging.info(\"Loading BSE script master from local file...\")\n",
        "        bse_master_df = pd.read_csv(bse_file)\n",
        "        if not bse_master_df.empty:\n",
        "            bse_master_df['token'] = bse_master_df['token'].astype(str)\n",
        "            bse_symbol_token_map = pd.Series(\n",
        "                bse_master_df.token.values,\n",
        "                index=bse_master_df.symbol\n",
        "            ).to_dict()\n",
        "            logging.info(\"BSE Symbol-to-Token map created.\")\n",
        "    else:\n",
        "        logging.warning(f\"'{bse_file}' not found. BSE lookups will fail.\")\n",
        "\n",
        "def get_order_book():\n",
        "    if not smartApi: return pd.DataFrame([{\"Error\": \"Not connected to API.\"}])\n",
        "    try:\n",
        "        order_data = smartApi.orderBook()\n",
        "        if order_data and order_data.get('status') and order_data.get('data') is not None:\n",
        "            return pd.DataFrame(order_data['data'])\n",
        "        else:\n",
        "            return pd.DataFrame([{\"Message\": order_data.get('message', 'Order book is empty or error occurred')}])\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error fetching order book: {e}\")\n",
        "        return pd.DataFrame([{\"Error\": str(e)}])\n",
        "\n",
        "def get_trade_book():\n",
        "    if not smartApi: return pd.DataFrame([{\"Error\": \"Not connected to API.\"}])\n",
        "    try:\n",
        "        trade_data = smartApi.tradeBook()\n",
        "        if trade_data and trade_data.get('status') and trade_data.get('data') is not None:\n",
        "            return pd.DataFrame(trade_data['data'])\n",
        "        else:\n",
        "            return pd.DataFrame([{\"Message\": trade_data.get('message', 'Trade book is empty or error occurred')}])\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error fetching trade book: {e}\")\n",
        "        return pd.DataFrame([{\"Error\": str(e)}])"
      ],
      "metadata": {
        "id": "qE2POzgUc_JF"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Dashboard**"
      ],
      "metadata": {
        "id": "r7ZekxdhSTrx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Helper Functions ---\n",
        "\n",
        "def _load_industry_data() -> pd.DataFrame | None:\n",
        "    \"\"\"\n",
        "    Loads the industry data from the CSV file.\n",
        "    This function is modified to always read from the file\n",
        "    to reflect changes made in the Watchlist Manager.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        logging.info(\"Loading industry data from Dashboard_Industry.csv.\")\n",
        "        df = pd.read_csv(\n",
        "            industry,\n",
        "            dtype={\"BSE Code\": str, \"NSE Code\": str}\n",
        "        )\n",
        "\n",
        "        # Clean the relevant columns once upon loading\n",
        "        for col in [\"BSE Code\", \"NSE Code\", \"Industry\"]:\n",
        "            if col in df.columns:\n",
        "                df[col] = df[col].astype(str).str.strip().str.upper()\n",
        "\n",
        "        # We are no longer caching this data persistently in a global variable\n",
        "        return df\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        logging.error(\"Dashboard_Industry.csv not found. Industry data will not be available.\")\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Failed to load or process Dashboard_Industry.csv: {e}\")\n",
        "        return None\n",
        "\n",
        "def _add_value_columns(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Adds 'invested_value' and 'current_value' columns if they don't exist.\n",
        "    This prevents redundant calculations in downstream functions.\n",
        "    \"\"\"\n",
        "    if \"invested_value\" not in df.columns:\n",
        "        df[\"invested_value\"] = pd.to_numeric(df[\"quantity\"], errors='coerce') * pd.to_numeric(df[\"averageprice\"], errors='coerce')\n",
        "    if \"current_value\" not in df.columns:\n",
        "        df[\"current_value\"] = pd.to_numeric(df[\"quantity\"], errors='coerce') * pd.to_numeric(df[\"ltp\"], errors='coerce')\n",
        "    return df\n",
        "\n",
        "def _create_placeholder_fig(message=\"No data to plot\"):\n",
        "    \"\"\"Creates a matplotlib figure with a text message.\"\"\"\n",
        "    fig, ax = plt.subplots()\n",
        "    ax.text(0.5, 0.5, message, ha='center', va='center', fontsize=12)\n",
        "    ax.axis('off')\n",
        "    plt.tight_layout()\n",
        "    return fig\n",
        "\n",
        "# ====== Main Portfolio Function (Refined) ======\n",
        "\n",
        "def get_portfolio_data():\n",
        "    \"\"\"\n",
        "    Fetches portfolio holdings, cleans the data, and merges it with industry information.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        portfolio_data = smartApi.holding()\n",
        "\n",
        "        # Simplified check for valid, non-empty data\n",
        "        if not (portfolio_data and portfolio_data.get('status') and portfolio_data.get('data')):\n",
        "            msg = portfolio_data.get('message', 'No holdings found or an API error occurred.')\n",
        "            logging.warning(msg)\n",
        "            return pd.DataFrame([{\"Message\": msg}])\n",
        "\n",
        "        df = pd.DataFrame(portfolio_data['data'])\n",
        "\n",
        "        # Ensure required columns exist\n",
        "        required_cols = [\n",
        "            \"tradingsymbol\", \"exchange\", \"symboltoken\", \"quantity\",\n",
        "            \"averageprice\", \"ltp\", \"profitandloss\", \"pnlpercentage\"\n",
        "        ]\n",
        "        if not all(col in df.columns for col in required_cols):\n",
        "            logging.error(\"API response is missing one or more required columns.\")\n",
        "            return pd.DataFrame([{\"Error\": \"Malformed API response from holdings endpoint.\"}])\n",
        "\n",
        "        df = df[required_cols]\n",
        "\n",
        "        # --- Refined Industry Mapping ---\n",
        "        # Load industry data fresh each time to ensure latest watchlist changes are reflected\n",
        "        industry_df = _load_industry_data()\n",
        "        if industry_df is not None:\n",
        "            # Prepare portfolio df for merging\n",
        "            df['bse_code_str'] = df['symboltoken'].astype(str).str.strip().str.upper()\n",
        "            df['nse_code_str'] = df['tradingsymbol'].str.replace(r'-[A-Z0-9]+$', '', regex=True).str.strip().str.upper()\n",
        "\n",
        "            # Merge based on exchange\n",
        "            df_nse = df[df['exchange'] == 'NSE']\n",
        "            df_bse = df[df['exchange'] == 'BSE']\n",
        "\n",
        "            merged_nse = pd.merge(df_nse, industry_df, left_on='nse_code_str', right_on='NSE Code', how='left')\n",
        "            merged_bse = pd.merge(df_bse, industry_df, left_on='bse_code_str', right_on='BSE Code', how='left')\n",
        "\n",
        "            df = pd.concat([merged_nse, merged_bse], ignore_index=True)\n",
        "            df['Industry'] = df['Industry'].fillna('N/A') # Fill missing industries\n",
        "        else:\n",
        "            df['Industry'] = 'N/A' # Add placeholder if industry file is missing\n",
        "\n",
        "        # --- Clean up and Reorder Columns ---\n",
        "        df.drop(\n",
        "            columns=['symboltoken', 'bse_code_str', 'nse_code_str', 'BSE Code', 'NSE Code', 'Name'],\n",
        "            inplace=True,\n",
        "            errors='ignore'\n",
        "        )\n",
        "\n",
        "        # Move 'Industry' column to be next to 'tradingsymbol'\n",
        "        if \"Industry\" in df.columns:\n",
        "            industry_col = df.pop(\"Industry\")\n",
        "            df.insert(loc=df.columns.get_loc(\"tradingsymbol\") + 1, column=\"Industry\", value=industry_col)\n",
        "\n",
        "        # Add invested and current value columns for the final table\n",
        "        df = _add_value_columns(df)\n",
        "\n",
        "        return df\n",
        "\n",
        "    except Exception as e:\n",
        "        logging.exception(\"An unhandled error occurred while fetching portfolio data.\")\n",
        "        return pd.DataFrame([{\"Error\": str(e)}])\n",
        "\n",
        "# ====== Dashboard Helper Functions (Refined) ======\n",
        "\n",
        "def calculate_metrics(df: pd.DataFrame):\n",
        "\n",
        "    if \"Error\" in df.columns or \"Message\" in df.columns:\n",
        "        return df # Pass through error/message dataframes\n",
        "\n",
        "    df = _add_value_columns(df.copy()) # Use .copy() to avoid SettingWithCopyWarning\n",
        "\n",
        "    total_symbols = df[\"tradingsymbol\"].nunique()\n",
        "    total_invested = df[\"invested_value\"].sum()\n",
        "    total_current = df[\"current_value\"].sum()\n",
        "    profit_loss = total_current - total_invested\n",
        "    pct_gain_loss = (profit_loss / total_invested) * 100 if total_invested else 0\n",
        "\n",
        "    summary = {\n",
        "        \"Total Unique Symbols\": int(total_symbols),\n",
        "        \"Total Invested (â‚¹)\": f\"{int(round(total_invested)):,}\",\n",
        "        \"Portfolio Value (â‚¹)\": f\"{int(round(total_current)):,}\",\n",
        "        \"Profit/Loss (â‚¹)\": f\"{int(round(profit_loss)):,}\",\n",
        "        \"P/L %\": f\"{pct_gain_loss:.2f}%\"\n",
        "    }\n",
        "    return pd.DataFrame([summary])\n",
        "\n",
        "\n",
        "def plot_charts(df: pd.DataFrame):\n",
        "\n",
        "    if \"Error\" in df.columns or \"Message\" in df.columns or \"Industry\" not in df.columns or df['Industry'].nunique() <= 1:\n",
        "        logging.warning(\"Cannot generate industry plots due to error, message, or insufficient data.\")\n",
        "        return _create_placeholder_fig(), _create_placeholder_fig()\n",
        "\n",
        "    df = _add_value_columns(df.copy()) # Use .copy()\n",
        "\n",
        "    industry_invested = df.groupby(\"Industry\")[\"invested_value\"].sum()\n",
        "    industry_current = df.groupby(\"Industry\")[\"current_value\"].sum()\n",
        "\n",
        "    # --- Pie Chart: Invested Value by Industry ---\n",
        "    fig1, ax1 = plt.subplots(figsize=(7, 6), constrained_layout=True)\n",
        "\n",
        "    def autopct_format(values):\n",
        "        def inner_autopct(pct):\n",
        "            total = sum(values)\n",
        "            val = int(round(pct * total / 100.0))\n",
        "            return f\"{pct:.1f}%\\n(â‚¹{val:,})\"\n",
        "        return inner_autopct\n",
        "\n",
        "    cmap = plt.get_cmap(\"tab20\")\n",
        "    colors = cmap.colors[:len(industry_invested)]\n",
        "\n",
        "    wedges, texts, autotexts = ax1.pie(\n",
        "        industry_invested,\n",
        "        colors=colors,\n",
        "        autopct=autopct_format(industry_invested),\n",
        "        startangle=90,\n",
        "        pctdistance=0.8,\n",
        "        textprops={'fontsize': 9}\n",
        "    )\n",
        "    ax1.set_title(\"Invested Value by Industry\", fontsize=14, weight=\"bold\")\n",
        "    ax1.legend(\n",
        "        wedges, industry_invested.index, title=\"Industry\",\n",
        "        loc=\"center left\", bbox_to_anchor=(1.05, 0.5), fontsize=9\n",
        "    )\n",
        "\n",
        "    # --- Bar Chart: PnL % by Industry (Replaces the old chart) ---\n",
        "    fig2, ax2 = plt.subplots(figsize=(8, 6), constrained_layout=True)\n",
        "\n",
        "    # Calculate PnL % for each industry\n",
        "    industry_pnl = ((industry_current - industry_invested) / industry_invested) * 100\n",
        "    industry_pnl = industry_pnl.replace([np.inf, -np.inf], 0).fillna(0) # Handle division by zero\n",
        "    industry_pnl = industry_pnl.sort_values(ascending=False)\n",
        "\n",
        "    colors = industry_pnl.apply(lambda x: 'green' if x >= 0 else 'red')\n",
        "\n",
        "    ax2.bar(industry_pnl.index, industry_pnl, color=colors)\n",
        "    ax2.set_title(\"Profit/Loss % by Industry\", fontsize=14, weight=\"bold\")\n",
        "    ax2.set_xticklabels(industry_pnl.index, rotation=45, ha=\"right\", fontsize=9)\n",
        "    ax2.set_ylabel(\"P/L %\")\n",
        "    ax2.set_xlabel(None)\n",
        "    ax2.axhline(0, color='black', linewidth=0.8) # Zero line\n",
        "    ax2.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "\n",
        "    if not industry_pnl.empty:\n",
        "        abs_max = max(abs(industry_pnl.min()), abs(industry_pnl.max()))\n",
        "        # Set the y-axis limits with a 10% buffer for spacing\n",
        "        ax2.set_ylim(-abs_max * 1.1, abs_max * 1.1)\n",
        "\n",
        "    return fig1, fig2\n",
        "\n",
        "def plot_pnl_bars(df: pd.DataFrame):\n",
        "\n",
        "    if \"Error\" in df.columns or \"Message\" in df.columns or \"pnlpercentage\" not in df.columns:\n",
        "        logging.warning(\"Cannot generate PnL bar chart due to error or missing data.\")\n",
        "        return _create_placeholder_fig()\n",
        "\n",
        "    # Ensure data is numeric and sorted for better visualization\n",
        "    df = df.copy()\n",
        "    df['pnlpercentage'] = pd.to_numeric(df['pnlpercentage'], errors='coerce').fillna(0)\n",
        "    df = df.sort_values('pnlpercentage', ascending=False)\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(max(8, len(df) * 0.6), 5), constrained_layout=True)\n",
        "    colors = df['pnlpercentage'].apply(lambda x: 'green' if x >= 0 else 'red')\n",
        "\n",
        "    ax.bar(df['tradingsymbol'], df['pnlpercentage'], color=colors)\n",
        "    ax.set_ylabel(\"Profit/Loss (%)\")\n",
        "    ax.set_title(\"PnL per Script\", fontsize=14, weight=\"bold\")\n",
        "    ax.axhline(0, color='black', linewidth=0.8)\n",
        "    ax.tick_params(axis='x', rotation=75, labelsize=9)\n",
        "    ax.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "\n",
        "    if not df['pnlpercentage'].empty:\n",
        "        pnl_data = df['pnlpercentage']\n",
        "        abs_max = max(abs(pnl_data.min()), abs(pnl_data.max()))\n",
        "        ax.set_ylim(-abs_max * 1.1, abs_max * 1.1)\n",
        "\n",
        "    return fig\n",
        "\n",
        "# ====== CSV Export Function ======\n",
        "\n",
        "def export_portfolio_csv(df: pd.DataFrame):\n",
        "\n",
        "    if df.empty or \"Error\" in df.columns or \"Message\" in df.columns:\n",
        "        logging.warning(\"Export aborted: Input DataFrame contains an error, a message, or is empty.\")\n",
        "        return None\n",
        "\n",
        "    try:\n",
        "        export_dir = \"downloadrohit\"\n",
        "        os.makedirs(export_dir, exist_ok=True)\n",
        "\n",
        "        filepath = os.path.join(export_dir, f\"dashboard_portfolio.csv\")\n",
        "\n",
        "        export_df = df.copy()\n",
        "\n",
        "        # Format currency columns for better readability in the CSV\n",
        "        for col in ['averageprice', 'ltp', 'profitandloss', 'invested_value', 'current_value']:\n",
        "            if col in export_df.columns:\n",
        "                export_df[col] = export_df[col].apply(lambda x: f'{x:,.2f}')\n",
        "\n",
        "        export_df.to_csv(filepath, index=False)\n",
        "\n",
        "        logging.info(f\"Successfully exported portfolio to {filepath}\")\n",
        "        return filepath\n",
        "\n",
        "    except Exception as e:\n",
        "        logging.exception(\"An error occurred during CSV export.\")\n",
        "        return None\n",
        "\n",
        "# ====== Master Analysis Function ======\n",
        "\n",
        "def analyze_portfolio():\n",
        "\n",
        "    df = get_portfolio_data()\n",
        "\n",
        "    # Critical: Ensure a consistent number of return values for the UI\n",
        "    if df.empty or \"Error\" in df.columns or \"Message\" in df.columns:\n",
        "        error_msg = df.iloc[0,0] if not df.empty else \"No portfolio data returned.\"\n",
        "        # Return 6 items: df, summary, fig1, fig2, fig3, filepath\n",
        "        return (\n",
        "            df,\n",
        "            pd.DataFrame([{\"Message\": error_msg}]),\n",
        "            _create_placeholder_fig(\"Industry chart not available\"),\n",
        "            _create_placeholder_fig(\"Value chart not available\"),\n",
        "            _create_placeholder_fig(\"PnL chart not available\"),\n",
        "            None  # No file path on error\n",
        "        )\n",
        "\n",
        "    summary_df = calculate_metrics(df)\n",
        "    fig1, fig2 = plot_charts(df)\n",
        "    fig3 = plot_pnl_bars(df)\n",
        "\n",
        "    # Generate the CSV file for download\n",
        "    csv_filepath = export_portfolio_csv(df)\n",
        "\n",
        "    # Round numerical columns for cleaner display in the UI Dataframe\n",
        "    display_cols_to_round = ['averageprice', 'ltp', 'profitandloss', 'pnlpercentage', 'invested_value', 'current_value']\n",
        "    for col in display_cols_to_round:\n",
        "        if col in df.columns:\n",
        "            # Ensure column is numeric before rounding\n",
        "            df[col] = pd.to_numeric(df[col], errors='coerce').round(2)\n",
        "\n",
        "    df = df.sort_values(by='tradingsymbol')\n",
        "\n",
        "    return df, summary_df, fig1, fig2, fig3, csv_filepath"
      ],
      "metadata": {
        "id": "BLQizTek5X5j"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Master portfolio**"
      ],
      "metadata": {
        "id": "L4FeoLaRiknq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "PORTFOLIO_FILE = master_portfolio\n",
        "ARCHIVE_FILE = archive\n",
        "PORTFOLIO_COLUMNS = ['date', 'symbol', 'parcel', 'quantity', 'avg_price', 'exchange', 'SWEET_Value']"
      ],
      "metadata": {
        "id": "Ej-WTEGifHqy"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_scrip_masters():\n",
        "    global nse_symbol_token_map, bse_symbol_token_map, nse_symbols_list, bse_symbols_list\n",
        "    exchange_map = {\n",
        "        'NSE': {'file': nse, 'map': nse_symbol_token_map, 'list': nse_symbols_list},\n",
        "        'BSE': {'file': bse, 'map': bse_symbol_token_map, 'list': bse_symbols_list}\n",
        "    }\n",
        "    for exchange, data in exchange_map.items():\n",
        "        if os.path.exists(data['file']):\n",
        "            logging.info(f\"Loading {exchange} scrip master from local file...\")\n",
        "            df = pd.read_csv(data['file'])\n",
        "            df['token'] = df['token'].astype(str)\n",
        "            current_map = pd.Series(df.token.values, index=df.symbol).to_dict()\n",
        "            data['map'].update(current_map)\n",
        "            exchange_map[exchange]['list'][:] = sorted(list(current_map.keys()))\n",
        "            logging.info(f\"{exchange} Symbol-to-Token map created with {len(exchange_map[exchange]['list'])} symbols.\")\n",
        "        else:\n",
        "            logging.warning(f\"'{data['file']}' not found. {exchange} lookups will fail.\")\n",
        "    nse_symbols_list[:] = exchange_map['NSE']['list']\n",
        "    bse_symbols_list[:] = exchange_map['BSE']['list']\n"
      ],
      "metadata": {
        "id": "j8luGNYCiuAB"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_api_data(api_function):\n",
        "    if not smartApi: return pd.DataFrame([{\"Error\": \"Not connected to API.\"}])\n",
        "    try:\n",
        "        data = api_function()\n",
        "        if data and data.get('status') and data.get('data') is not None:\n",
        "            return pd.DataFrame(data['data'])\n",
        "        return pd.DataFrame([{\"Message\": data.get('message', 'Data is empty or error occurred')}])\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error fetching data: {e}\")\n",
        "        return pd.DataFrame([{\"Error\": str(e)}])\n",
        "\n",
        "get_trade_book = lambda: get_api_data(smartApi.tradeBook)\n",
        "get_holdings = lambda: get_api_data(smartApi.holding)\n",
        "\n",
        "# UPDATED CODE\n",
        "def get_ltp(symbol, token, exchange):\n",
        "    if not smartApi: return 0\n",
        "    try:\n",
        "        # Try getting the token from the symbol maps first, falling back to the provided token if not found\n",
        "        # This ensures we use the correct token from our loaded masters\n",
        "        token_from_map = nse_symbol_token_map.get(symbol)\n",
        "        if token_from_map is None:\n",
        "            token_from_map = bse_symbol_token_map.get(symbol)\n",
        "\n",
        "        # Use the token from the map if found, otherwise use the one passed into the function\n",
        "        actual_token = token_from_map if token_from_map is not None else token\n",
        "\n",
        "        # Ensure token is a string for the API call\n",
        "        if actual_token is None:\n",
        "             logging.warning(f\"No token found for symbol {symbol} on exchange {exchange}.\")\n",
        "             return 0\n",
        "\n",
        "        actual_token_str = str(actual_token)\n",
        "\n",
        "        response = smartApi.ltpData(exchange, symbol, actual_token_str)\n",
        "        if response and response.get('status') and response.get('data'):\n",
        "            return response['data']['ltp']\n",
        "\n",
        "        # Log the API response message if status is False or data is None\n",
        "        if response and response.get('message'):\n",
        "             logging.error(f\"API Error fetching LTP for {symbol} (Token: {actual_token_str}): {response.get('message')}\")\n",
        "        else:\n",
        "             logging.error(f\"Unknown API response for LTP for {symbol} (Token: {actual_token_str}): {response}\")\n",
        "\n",
        "        return 0\n",
        "    except Exception as e:\n",
        "        logging.exception(f\"Exception fetching LTP for {symbol} (Token: {actual_token_str if 'actual_token_str' in locals() else 'N/A'}): {e}\")\n",
        "        return 0"
      ],
      "metadata": {
        "id": "Gjl1GwKydOSN"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_portfolio_from_csv(file_path=PORTFOLIO_FILE):\n",
        "    if not os.path.exists(file_path):\n",
        "        return pd.DataFrame(columns=PORTFOLIO_COLUMNS)\n",
        "    try:\n",
        "        df = pd.read_csv(file_path)\n",
        "        return df.sort_values(by=['symbol', 'parcel']).reset_index(drop=True)\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error loading CSV '{file_path}': {e}\")\n",
        "        return pd.DataFrame(columns=PORTFOLIO_COLUMNS)\n",
        "\n",
        "def load_archive_from_csv(file_path):\n",
        "    if not os.path.exists(file_path):\n",
        "        return pd.DataFrame(columns=PORTFOLIO_COLUMNS)\n",
        "    try:\n",
        "        df = pd.read_csv(file_path)\n",
        "        return df.sort_values(by=['symbol', 'parcel']).reset_index(drop=True)\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error loading CSV '{file_path}': {e}\")\n",
        "        return pd.DataFrame(columns=PORTFOLIO_COLUMNS)\n",
        "\n",
        "def save_portfolio_to_csv(df, file_path=PORTFOLIO_FILE):\n",
        "    try:\n",
        "        # Ensure all required columns exist, adding them if necessary\n",
        "        for col in PORTFOLIO_COLUMNS:\n",
        "            if col not in df.columns:\n",
        "                df[col] = None\n",
        "        # Reorder columns to maintain consistency\n",
        "        df[PORTFOLIO_COLUMNS].to_csv(file_path, index=False)\n",
        "        logging.info(f\"Portfolio saved to {file_path}\")\n",
        "        return f\"âœ… Successfully saved {len(df)} rows to {file_path}.\"\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error saving portfolio: {e}\")\n",
        "        return f\"â—ï¸ Error saving portfolio: {e}\"\n",
        "\n",
        "def archive_entry(df_entry):\n",
        "    try:\n",
        "        df_entry.to_csv(ARCHIVE_FILE, mode='a', header=not os.path.exists(ARCHIVE_FILE), index=False)\n",
        "        logging.info(f\"Archived {len(df_entry)} entr(y/ies) to {ARCHIVE_FILE}\")\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Failed to archive entry: {e}\")\n"
      ],
      "metadata": {
        "id": "oTPL1F7EdlW9"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def download_holdings_as_master_portfolio():\n",
        "    holdings_df = get_holdings()\n",
        "    if holdings_df.empty or \"Error\" in holdings_df.columns or \"Message\" in holdings_df.columns:\n",
        "        return \"Could not download holdings or portfolio is empty.\", pd.DataFrame()\n",
        "\n",
        "    rows = [{\n",
        "        'date': datetime.now().strftime('%Y-%m-%d'),\n",
        "        'symbol': item['tradingsymbol'],\n",
        "        'parcel': 'P1',\n",
        "        'quantity': int(item['quantity']),\n",
        "        'avg_price': float(item['averageprice']),\n",
        "        'exchange': item['exchange'],\n",
        "        'token': item['symboltoken']\n",
        "    } for _, item in holdings_df.iterrows()]\n",
        "\n",
        "    df_to_save = pd.DataFrame(rows)\n",
        "    status = save_portfolio_to_csv(df_to_save)\n",
        "    return f\"Successfully created master portfolio. {status}\", df_to_save\n",
        "\n",
        "def check_mismatches():\n",
        "    master_df = load_portfolio_from_csv()\n",
        "    angel_df = get_holdings()\n",
        "    alerts = []\n",
        "\n",
        "    if \"Error\" in angel_df.columns:\n",
        "        return \"âŒ Could not fetch Angel One holdings to check for mismatches.\"\n",
        "\n",
        "    # --- NEW LOGIC TO CALCULATE NET POSITION ---\n",
        "    master_summary_list = []\n",
        "    # Group by stock symbol to process each one individually\n",
        "    for symbol, group in master_df.groupby('symbol'):\n",
        "        # Calculate total buys (parcels starting with 'P')\n",
        "        total_p = group[group['parcel'].str.startswith('P')]['quantity'].sum()\n",
        "        # Calculate total sells (parcels starting with 'S')\n",
        "        total_s = group[group['parcel'].str.startswith('S')]['quantity'].sum()\n",
        "        # Calculate the net position\n",
        "        net_quantity = total_p - total_s\n",
        "        master_summary_list.append({'symbol': symbol, 'master_qty': net_quantity})\n",
        "\n",
        "    if not master_summary_list:\n",
        "        master_summary = pd.DataFrame(columns=['symbol', 'master_qty'])\n",
        "    else:\n",
        "        master_summary = pd.DataFrame(master_summary_list)\n",
        "\n",
        "    # Prepare Angel One holdings data\n",
        "    if angel_df.empty:\n",
        "         angel_summary = pd.DataFrame(columns=['symbol', 'angel_qty'])\n",
        "    else:\n",
        "        angel_summary = angel_df[['tradingsymbol', 'quantity']].copy()\n",
        "        angel_summary['quantity'] = pd.to_numeric(angel_summary['quantity'])\n",
        "        angel_summary.rename(columns={'tradingsymbol': 'symbol', 'quantity': 'angel_qty'}, inplace=True)\n",
        "\n",
        "    # Merge the calculated net positions with the broker's data\n",
        "    comparison_df = pd.merge(master_summary, angel_summary, on='symbol', how='outer').fillna(0)\n",
        "\n",
        "    for _, row in comparison_df.iterrows():\n",
        "        master_qty = int(row['master_qty'])\n",
        "        angel_qty = int(row['angel_qty'])\n",
        "\n",
        "        if master_qty != angel_qty:\n",
        "            symbol = row['symbol']\n",
        "\n",
        "            # Get the full parcel details for the mismatched stock\n",
        "            stock_details_df = master_df[master_df['symbol'] == symbol]\n",
        "\n",
        "            # Create the breakdown string (e.g., \"P1: 100, P2: 180, S2: 140\")\n",
        "            parcel_breakdown = \", \".join([\n",
        "                f\"{detail_row['parcel']}: {int(detail_row['quantity'])}\"\n",
        "                for _, detail_row in stock_details_df.iterrows()\n",
        "            ])\n",
        "\n",
        "            if not parcel_breakdown:\n",
        "                parcel_breakdown = \"No holdings in master file\"\n",
        "\n",
        "            # Construct the new, correct alert message\n",
        "            alert_message = (\n",
        "                f\"- **{symbol}**: Mismatch!\\n\"\n",
        "                f\"  - Master Portfolio Net: **{master_qty}** ({parcel_breakdown})\\n\"\n",
        "                f\"  - Angel One Holdings: **{angel_qty}**\"\n",
        "            )\n",
        "            alerts.append(alert_message)\n",
        "\n",
        "    if not alerts:\n",
        "        return \"âœ… No mismatches found between Master Portfolio and Angel One holdings.\"\n",
        "\n",
        "    return \"ðŸš¨ **Mismatch Alert!**\\n\\n\" + \"\\n\\n\".join(alerts)\n"
      ],
      "metadata": {
        "id": "3Jecmf_dfRhc"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preview_trades_from_tradebook():\n",
        "    trades_df = get_trade_book()\n",
        "    trades_df['fillsize'] = pd.to_numeric(trades_df['fillsize'], errors='coerce')\n",
        "    trades_df['fillprice'] = pd.to_numeric(trades_df['fillprice'], errors='coerce')\n",
        "\n",
        "    if trades_df.empty or 'transactiontype' not in trades_df.columns:\n",
        "        return pd.DataFrame(), \"Trade book is empty or could not be fetched.\"\n",
        "\n",
        "    # Load the current master portfolio to check existing parcels\n",
        "    master_portfolio_df = load_portfolio_from_csv()\n",
        "    # Load SWEET lookup table once\n",
        "    sweet_df = load_sweet_code()\n",
        "\n",
        "    # --- New Consolidation Logic ---\n",
        "    # Group trades by symbol and transaction type first to determine parcels correctly\n",
        "    consolidated_trades = []\n",
        "    for (symbol, tx_type), group in trades_df.groupby(['tradingsymbol', 'transactiontype']):\n",
        "        exchange = group['exchange'].iloc[0] # Assume exchange is consistent for a symbol/tx_type group\n",
        "\n",
        "        # Determine the parcel based on transaction type and existing parcels\n",
        "        parcel = None\n",
        "        existing_parcels = master_portfolio_df[master_portfolio_df['symbol'] == symbol]['parcel'].tolist()\n",
        "\n",
        "        if tx_type == 'BUY':\n",
        "            if 'P1' not in existing_parcels: parcel = 'P1'\n",
        "            elif 'P2' not in existing_parcels: parcel = 'P2'\n",
        "            elif 'P3' not in existing_parcels: parcel = 'P3'\n",
        "            elif 'P4' not in existing_parcels: parcel = 'P4'\n",
        "            else: parcel = 'P1' # Default if all P parcels exist\n",
        "\n",
        "            # Consolidate multiple BUY entries for the same parcel\n",
        "            total_qty = group['fillsize'].sum()\n",
        "            total_value = (group['fillsize'] * group['fillprice']).sum()\n",
        "            avg_price = total_value / total_qty if total_qty > 0 else 0\n",
        "\n",
        "            if total_qty > 0: # Only add if there's a quantity\n",
        "                 sweet_value = sweet_value_for_symbol(symbol, master_portfolio_df, sweet_df) if parcel == 'P1' else None\n",
        "\n",
        "                 consolidated_trades.append({\n",
        "                    'date': datetime.now().strftime('%d-%m-%Y'), # Format date as DD-MM-YYYY\n",
        "                    'symbol': symbol,\n",
        "                    'parcel': parcel,\n",
        "                    'quantity': int(total_qty),\n",
        "                    'avg_price': round(avg_price, 2),\n",
        "                    'exchange': exchange,\n",
        "                    'SWEET_Value': sweet_value\n",
        "                 })\n",
        "\n",
        "        elif tx_type == 'SELL':\n",
        "            # Determine SELL parcel (S1, S2, S3, S4) based on the highest existing P parcel\n",
        "            if 'P4' in existing_parcels: parcel = 'S4'\n",
        "            elif 'P3' in existing_parcels: parcel = 'S3'\n",
        "            elif 'P2' in existing_parcels: parcel = 'S2'\n",
        "            elif 'P1' in existing_parcels: parcel = 'S1'\n",
        "            else: parcel = 'S1' # Default if no P parcels exist\n",
        "\n",
        "            # For SELLs, list each trade book entry as a separate row in the preview,\n",
        "            # but still determine the parcel based on the *current* master portfolio state.\n",
        "            for _, trade in group.iterrows():\n",
        "                 consolidated_trades.append({\n",
        "                    'date': datetime.now().strftime('%d-%m-%Y'), # Format date as DD-MM-YYYY\n",
        "                    'symbol': symbol,\n",
        "                    'parcel': parcel, # Use the determined parcel for all sell trades in this group\n",
        "                    'quantity': int(trade['fillsize']),\n",
        "                    'avg_price': float(trade['fillprice']),\n",
        "                    'exchange': exchange,\n",
        "                    'SWEET_Value': None # SWEET_Value is only for BUY P1 entries\n",
        "                 })\n",
        "\n",
        "\n",
        "    preview_df = pd.DataFrame(consolidated_trades)\n",
        "    return preview_df, f\"Found {len(preview_df)} consolidated entries from trade book with calculated parcels.\"\n",
        "\n",
        "\n",
        "def add_preview_to_portfolio(current_portfolio_df, preview_df):\n",
        "    if preview_df.empty:\n",
        "        return current_portfolio_df, \"Preview is empty, nothing to add.\"\n",
        "\n",
        "    # Ensure columns match before concatenating\n",
        "    # Drop 'Delete' from current_portfolio_df temporarily if it exists\n",
        "    portfolio_cols_no_delete = [col for col in current_portfolio_df.columns if col != 'Delete']\n",
        "    current_portfolio_subset = current_portfolio_df[portfolio_cols_no_delete]\n",
        "\n",
        "    # Ensure preview_df has all necessary columns, add missing ones with None\n",
        "    for col in current_portfolio_subset.columns:\n",
        "        if col not in preview_df.columns:\n",
        "            preview_df[col] = None\n",
        "\n",
        "    # Ensure columns are in the same order and have compatible dtypes\n",
        "    preview_df = preview_df[current_portfolio_subset.columns]\n",
        "    # Attempt to cast columns to match current_portfolio_subset\n",
        "    for col in current_portfolio_subset.columns:\n",
        "        try:\n",
        "            preview_df[col] = preview_df[col].astype(current_portfolio_subset[col].dtype)\n",
        "        except Exception as e:\n",
        "            logging.warning(f\"Could not cast column {col} in preview_df: {e}\")\n",
        "\n",
        "\n",
        "    combined_df = pd.concat([current_portfolio_subset, preview_df], ignore_index=True)\n",
        "\n",
        "    # Re-add the 'Delete' column if it was present initially\n",
        "    if 'Delete' in current_portfolio_df.columns:\n",
        "        combined_df['Delete'] = False\n",
        "        combined_df = combined_df[PORTFOLIO_COLUMNS + ['Delete']] # Ensure order\n",
        "\n",
        "    return combined_df.sort_values(by=['symbol', 'parcel']).reset_index(drop=True), \"âœ… Entries added to portfolio editor. Please review and save.\"\n",
        "\n",
        "\n",
        "def load_and_display_portfolio():\n",
        "\n",
        "    df = load_portfolio_from_csv() # It uses your existing function to get the data\n",
        "    df[\"avg_price\"] = df[\"avg_price\"].round(2)\n",
        "\n",
        "    if not df.empty:\n",
        "        df['Delete'] = False\n",
        "        # Reorder columns to have 'Delete' at the end for consistency\n",
        "        df = df[PORTFOLIO_COLUMNS + ['Delete']]\n",
        "    else:\n",
        "         # If file doesn't exist, return an empty DataFrame with all columns\n",
        "        df = pd.DataFrame(columns=PORTFOLIO_COLUMNS + ['Delete'])\n",
        "\n",
        "    return df, f\"Loaded {len(df)} rows from {PORTFOLIO_FILE}.\"\n",
        "\n",
        "def delete_and_save_rows(df):\n",
        "\n",
        "    if 'Delete' not in df.columns or df['Delete'].sum() == 0:\n",
        "        return df, \"No rows were selected for deletion.\"\n",
        "\n",
        "    rows_to_delete_count = int(df['Delete'].sum())\n",
        "\n",
        "    # Create a new dataframe keeping only the rows where 'Delete' is False\n",
        "    cleaned_df = df[df['Delete'] == False].copy()\n",
        "\n",
        "    # Drop the temporary 'Delete' column before saving\n",
        "    cleaned_df.drop(columns=['Delete'], inplace=True)\n",
        "\n",
        "    # Save the cleaned dataframe to the CSV file\n",
        "    save_status = save_portfolio_to_csv(cleaned_df)\n",
        "\n",
        "    # Prepare the dataframe to be displayed back in the UI (with the Delete column)\n",
        "    if not cleaned_df.empty:\n",
        "        cleaned_df['Delete'] = False\n",
        "\n",
        "    status_message = f\"âœ… Deleted {rows_to_delete_count} row(s). {save_status}\"\n",
        "\n",
        "    return cleaned_df, status_message"
      ],
      "metadata": {
        "id": "PlVtyRyUfoaY"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def archive_matched_trades(portfolio_df):\n",
        "    \"\"\"Processes the portfolio DataFrame to archive matched trades.\"\"\"\n",
        "    if portfolio_df is None or portfolio_df.empty:\n",
        "        return \"No portfolio data to process.\"\n",
        "\n",
        "    df = portfolio_df.copy()\n",
        "    df.to_csv(\"downloadrohit/backup/master_portfolio.csv\", index=False)\n",
        "    df['quantity'] = pd.to_numeric(df['quantity'], errors='coerce').fillna(0)\n",
        "    df['parcel_type'] = df['parcel'].str[0]\n",
        "    df['parcel_num'] = df['parcel'].str[1:]\n",
        "\n",
        "    archive_temp_df = pd.read_csv(archive)\n",
        "    archive_temp_df.to_csv(\"downloadrohit/backup/archive.csv\", index=False)\n",
        "\n",
        "    ps_parcels = df[df['parcel_type'].isin(['P', 'S']) & df['parcel_num'].str.isdigit()].copy()\n",
        "    other_parcels = df[~df.index.isin(ps_parcels.index)]\n",
        "\n",
        "    if not ps_parcels.empty:\n",
        "        ps_parcels['signed_qty'] = np.where(ps_parcels['parcel_type'] == 'P', ps_parcels['quantity'], -ps_parcels['quantity'])\n",
        "        ps_parcels['net_qty'] = ps_parcels.groupby(['symbol', 'parcel_num'])['signed_qty'].transform('sum')\n",
        "        is_archivable = np.isclose(ps_parcels['net_qty'], 0)\n",
        "        rows_to_archive_df = ps_parcels[is_archivable]\n",
        "        rows_to_keep_ps = ps_parcels[~is_archivable]\n",
        "    else:\n",
        "        rows_to_archive_df = pd.DataFrame()\n",
        "        rows_to_keep_ps = pd.DataFrame()\n",
        "\n",
        "    updated_portfolio_df = pd.concat([rows_to_keep_ps, other_parcels])\n",
        "\n",
        "    cols_to_drop = ['parcel_type', 'parcel_num', 'signed_qty', 'net_qty']\n",
        "    rows_to_archive_df = rows_to_archive_df.drop(columns=cols_to_drop, errors='ignore')\n",
        "    updated_portfolio_df = updated_portfolio_df.drop(columns=cols_to_drop, errors='ignore')\n",
        "\n",
        "    archived_count = 0\n",
        "    if not rows_to_archive_df.empty:\n",
        "        archive_entry(rows_to_archive_df[PORTFOLIO_COLUMNS])\n",
        "        archived_count = len(rows_to_archive_df)\n",
        "\n",
        "    if not updated_portfolio_df.empty:\n",
        "        updated_portfolio_df[PORTFOLIO_COLUMNS].to_csv(PORTFOLIO_FILE, index=False)\n",
        "    else:\n",
        "        pd.DataFrame(columns=PORTFOLIO_COLUMNS).to_csv(PORTFOLIO_FILE, index=False)\n",
        "\n",
        "    return f\"âœ… Process complete! Archived {archived_count} rows. The portfolio file has been updated.\"\n",
        "\n",
        "# --- Wrapper Function for Gradio ---\n",
        "def run_archiving_process():\n",
        "    \"\"\"\n",
        "    This function acts as a bridge between the Gradio button and your main script.\n",
        "    It handles file loading and error checking.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        yield \"Processing... Please wait.\" # Gives immediate feedback\n",
        "        time.sleep(SLEEP*5) # Small delay to make the \"Processing\" message visible\n",
        "        df = pd.read_csv(PORTFOLIO_FILE)\n",
        "        status_message = archive_matched_trades(df)\n",
        "        yield status_message\n",
        "    except FileNotFoundError:\n",
        "        yield f\"âŒ ERROR: The file '{PORTFOLIO_FILE}' was not found. Please make sure it's in the same directory.\"\n",
        "    except Exception as e:\n",
        "        yield f\"âŒ An unexpected error occurred: {e}\"\n",
        "\n",
        "\n",
        "    # archive_button = gr.Button(\"ðŸš€ Run Archiving Process\", variant=\"primary\")\n",
        "\n",
        "    # # Connect the button click to the wrapper function\n",
        "    # archive_button.click(\n",
        "    #     fn=run_archiving_process,\n",
        "    #     inputs=None,\n",
        "    #     outputs=status_textbox\n",
        "    # )\n"
      ],
      "metadata": {
        "id": "R9hkJoT0nGSS"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Automated Buy and Sell**"
      ],
      "metadata": {
        "id": "yl6Gf4YcetKG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------- Helper functions ----------\n",
        "def _parse_date_safe(d):\n",
        "    \"\"\"Parse a date in DD-MM-YYYY or ISO format robustly. Return datetime.date or None.\"\"\"\n",
        "    if pd.isna(d):\n",
        "        return None\n",
        "    if isinstance(d, datetime):\n",
        "        return d.date()\n",
        "    s = str(d).strip()\n",
        "    for fmt in (\"%d-%m-%Y\", \"%Y-%m-%d\", \"%d/%m/%Y\", \"%d-%b-%Y\"):\n",
        "        try:\n",
        "            return datetime.strptime(s, fmt).date()\n",
        "        except Exception:\n",
        "            continue\n",
        "    # fallback: try pandas\n",
        "    try:\n",
        "        return pd.to_datetime(s, dayfirst=True).date()\n",
        "    except Exception:\n",
        "        logging.warning(f\"Could not parse date: {d}\")\n",
        "        return None\n",
        "\n",
        "def days_since(date_obj):\n",
        "    \"\"\"Return integer days since date_obj (date or string).\"\"\"\n",
        "    if date_obj is None:\n",
        "        return None\n",
        "    if not isinstance(date_obj, (datetime, )):\n",
        "        d = _parse_date_safe(date_obj)\n",
        "    else:\n",
        "        d = date_obj.date()\n",
        "    if d is None:\n",
        "        return None\n",
        "    return (datetime.now().date() - d).days\n",
        "\n",
        "def compute_pct_change(from_price, to_price):\n",
        "    \"\"\"Return percent change (to_price - from_price)/from_price * 100; safe on zero.\"\"\"\n",
        "    try:\n",
        "        if from_price == 0 or pd.isna(from_price):\n",
        "            return None\n",
        "        return ((to_price - from_price) / float(from_price)) * 100.0\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "# ---------- Read rules (backward compatible) ----------\n",
        "def read_rules(file_path=rule_book):\n",
        "    \"\"\"\n",
        "    Reads rule_book CSV and returns:\n",
        "      - a dict mapping variable_name -> numeric 'value' (backwards compatible),\n",
        "      - stores raw df under key '_df' for richer lookup,\n",
        "      - stores symbol overrides under key '_overrides' (dict mapping symbol->{variable_name: row_dict})\n",
        "    Old callers that used rules.get('S1', default) will continue to work.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        df = pd.read_csv(file_path, dtype=str).fillna('')\n",
        "        # ensure numeric fields coerced where relevant\n",
        "        numeric_cols = ['value', 'DaysDiff', 'Quantity']\n",
        "        for c in numeric_cols:\n",
        "            if c in df.columns:\n",
        "                df[c] = pd.to_numeric(df[c], errors='coerce')\n",
        "\n",
        "        # Build basic mapping: variable_name -> value (numeric) for backward compatibility\n",
        "        var_to_val = {}\n",
        "        for _, r in df.iterrows():\n",
        "            vn = r.get('variable_name', '')\n",
        "            if not vn:\n",
        "                continue\n",
        "            v = r.get('value')\n",
        "            try:\n",
        "                var_to_val[vn] = float(v) if not pd.isna(v) else None\n",
        "            except Exception:\n",
        "                var_to_val[vn] = None\n",
        "\n",
        "        # Build overrides mapping: Exception (symbol) -> {variable_name: row_dict}\n",
        "        overrides = {}\n",
        "        for _, r in df.iterrows():\n",
        "            exc = str(r.get('Exception', '')).strip()\n",
        "            if exc == '' or pd.isna(exc):\n",
        "                continue\n",
        "            if exc not in overrides:\n",
        "                overrides[exc] = {}\n",
        "            overrides[exc][r['variable_name']] = r.to_dict()\n",
        "\n",
        "        # Keep full df for lookups\n",
        "        var_to_val['_df'] = df\n",
        "        var_to_val['_overrides'] = overrides\n",
        "        return var_to_val\n",
        "\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error reading '{file_path}': {e}. Using defaults.\")\n",
        "        # Provide minimal defaults to maintain prior behavior\n",
        "        defaults = {\n",
        "            'P2': 5, 'P3': 10, 'P4': 20,\n",
        "            'S1': 30, 'S1Day': 15, 'SWEET_Diff': 5,\n",
        "            'S2': 100, 'S3': 100, 'S4': 100,\n",
        "            '_df': pd.DataFrame(), '_overrides': {}\n",
        "        }\n",
        "        return defaults\n",
        "\n",
        "def get_rule(rules, symbol, variable_name):\n",
        "    \"\"\"\n",
        "    Return a dict-like row for the requested rule.\n",
        "    If an override exists for the symbol (rules['_overrides']), use it.\n",
        "    Otherwise, return the generic row from rules['_df'] where variable_name matches.\n",
        "    The returned object is a dict with keys like 'value', 'DaysDiff', 'Quantity' (floats if present).\n",
        "    If not found, returns a default dict with 'value' from rules.get(variable_name).\n",
        "    \"\"\"\n",
        "    try:\n",
        "        overrides = rules.get('_overrides', {})\n",
        "        df = rules.get('_df', pd.DataFrame())\n",
        "        if symbol in overrides and variable_name in overrides[symbol]:\n",
        "            row = overrides[symbol][variable_name]\n",
        "            # normalize numeric fields\n",
        "            for k in ['value', 'DaysDiff', 'Quantity']:\n",
        "                if k in row and (row[k] is not None and row[k] != ''):\n",
        "                    try:\n",
        "                        row[k] = float(row[k])\n",
        "                    except Exception:\n",
        "                        row[k] = None\n",
        "            return row\n",
        "\n",
        "        # find generic\n",
        "        if isinstance(df, pd.DataFrame) and not df.empty:\n",
        "            matched = df[df['variable_name'] == variable_name]\n",
        "            if not matched.empty:\n",
        "                r = matched.iloc[0].to_dict()\n",
        "                for k in ['value', 'DaysDiff', 'Quantity']:\n",
        "                    if k in r and (r[k] is not None and r[k] != ''):\n",
        "                        try:\n",
        "                            r[k] = float(r[k])\n",
        "                        except Exception:\n",
        "                            r[k] = None\n",
        "                return r\n",
        "\n",
        "        # fallback: return numeric value and None for others\n",
        "        return {\n",
        "            'variable_name': variable_name,\n",
        "            'value': rules.get(variable_name),\n",
        "            'DaysDiff': None,\n",
        "            'Quantity': None,\n",
        "            'Exception': None\n",
        "        }\n",
        "    except Exception as e:\n",
        "        logging.error(f\"get_rule error: {e}\")\n",
        "        return {'variable_name': variable_name, 'value': rules.get(variable_name), 'DaysDiff': None, 'Quantity': None}\n",
        "\n",
        "def load_sweet_code(file_path=SWEET_Code_path):\n",
        "    \"\"\"Load SWEET_Code csv to map codes to SWEET_Value. Returns DataFrame.\"\"\"\n",
        "    try:\n",
        "        df = pd.read_csv(file_path, dtype=str).fillna('')\n",
        "        if 'SWEET_Value' in df.columns:\n",
        "            df['SWEET_Value'] = pd.to_numeric(df['SWEET_Value'], errors='coerce')\n",
        "        return df\n",
        "    except Exception as e:\n",
        "        logging.warning(f\"Unable to load SWEET_Code: {e}\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "def sweet_value_for_symbol(symbol, master_df=None, sweet_df=None):\n",
        "    \"\"\"\n",
        "    Get current SWEET_Value for a symbol:\n",
        "      - Prefer latest entry in master_portfolio for that symbol (if present)\n",
        "      - Else try lookup in sweet_df by matching NSE_Code or BSE_Code\n",
        "      - Else return None\n",
        "    Handles '-EQ' suffix difference in NSE_Code.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        if master_df is not None and not master_df.empty:\n",
        "            srows = master_df[master_df['symbol'] == symbol]\n",
        "            if not srows.empty:\n",
        "                # take last non-null SWEET_Value from master_portfolio rows for symbol\n",
        "                vals = srows['SWEET_Value'].dropna().values\n",
        "                if len(vals) > 0:\n",
        "                    return float(vals[-1])\n",
        "        if sweet_df is None:\n",
        "            sweet_df = load_sweet_code()\n",
        "        if not sweet_df.empty:\n",
        "            # try direct match in NSE_Code/BSE_Code columns\n",
        "            for col in ['NSE_Code', 'BSE_Code']: # Only check NSE_Code and BSE_Code in SWEET_Code.csv\n",
        "                if col in sweet_df.columns:\n",
        "                    # Remove '-EQ' from symbol if present for comparison with NSE_Code\n",
        "                    lookup_symbol = symbol.replace('-EQ', '') if col == 'NSE_Code' else symbol\n",
        "                    m = sweet_df[sweet_df[col] == lookup_symbol]\n",
        "                    if not m.empty and 'SWEET_Value' in m.columns:\n",
        "                        v = m.iloc[0]['SWEET_Value']\n",
        "                        if not pd.isna(v):\n",
        "                            return float(v)\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        logging.error(f\"sweet_value_for_symbol error: {e}\")\n",
        "        return None\n",
        "def current_sweet_value_for_symbol(symbol, sweet_df=None):\n",
        "    \"\"\"\n",
        "    Get current SWEET_Value for a symbol:\n",
        "      try lookup in sweet_df by matching NSE_Code or BSE_Code\n",
        "      - Else return None\n",
        "    Handles '-EQ' suffix difference in NSE_Code.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        if sweet_df is None:\n",
        "            sweet_df = load_sweet_code()\n",
        "        if not sweet_df.empty:\n",
        "            # try direct match in NSE_Code/BSE_Code columns\n",
        "            for col in ['NSE_Code', 'BSE_Code']: # Only check NSE_Code and BSE_Code in SWEET_Code.csv\n",
        "                if col in sweet_df.columns:\n",
        "                    # Remove '-EQ' from symbol if present for comparison with NSE_Code\n",
        "                    lookup_symbol = symbol.replace('-EQ', '') if col == 'NSE_Code' else symbol\n",
        "                    m = sweet_df[sweet_df[col] == lookup_symbol]\n",
        "                    if not m.empty and 'SWEET_Value' in m.columns:\n",
        "                        v = m.iloc[0]['SWEET_Value']\n",
        "                        if not pd.isna(v):\n",
        "                            return float(v)\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        logging.error(f\"sweet_value_for_symbol error: {e}\")\n",
        "        return None\n",
        "\n",
        "def current_active_parcels(master_df, symbol):\n",
        "    \"\"\"\n",
        "    Return dict of active parcel quantities and the first-row details for each parcel present for the symbol.\n",
        "    This function is defensive: sums quantities for each parcel and subtracts sell parcels S1..S4 to compute net.\n",
        "    Returns:\n",
        "      {\n",
        "        'quantities': { 'P1': net_qty, 'P2': net_qty, ... },\n",
        "        'rows': { 'P1': row_series_of_first_P1, ... }  # first matching row for avg_price/date/token/exchange\n",
        "      }\n",
        "    \"\"\"\n",
        "    if master_df is None or master_df.empty:\n",
        "        return {'quantities': {}, 'rows': {}}\n",
        "    grp = master_df[master_df['symbol'] == symbol]\n",
        "    if grp.empty:\n",
        "        return {'quantities': {}, 'rows': {}}\n",
        "\n",
        "    # sum up quantities for each parcel label\n",
        "    qtys = {}\n",
        "    rows = {}\n",
        "    for _, r in grp.iterrows():\n",
        "        parcel = r.get('parcel', '')\n",
        "        try:\n",
        "            q = float(r.get('quantity', 0))\n",
        "        except Exception:\n",
        "            q = 0.0\n",
        "        qtys[parcel] = qtys.get(parcel, 0.0) + q\n",
        "        # store first row details for parcel if not stored\n",
        "        if parcel.startswith('P') and parcel not in rows:\n",
        "            rows[parcel] = r\n",
        "        # But also store S rows maybe used to reduce net\n",
        "        if parcel.startswith('S') and parcel not in rows:\n",
        "            rows[parcel] = r\n",
        "\n",
        "    # compute net: for each P* compute net = Pn - Sn (if Sn exists)\n",
        "    net = {}\n",
        "    for n in ['P1', 'P2', 'P3', 'P4']:\n",
        "        pqty = qtys.get(n, 0.0)\n",
        "        s_label = 'S' + n[1]\n",
        "        sqty = qtys.get(s_label, 0.0)\n",
        "        net_qty = pqty - sqty\n",
        "        # Defensive: if both P4 and S4 exist, treat present status as P3 only (following your rule).\n",
        "        # That means if P4>0 and S4>0 then logically we should treat it as matched; so set net for P4 to 0\n",
        "        if n == 'P4' and qtys.get('P4', 0) > 0 and qtys.get('S4', 0) > 0:\n",
        "            # treat as if P4 matched -> present status should be P3\n",
        "            net_qty = 0.0\n",
        "        net[n] = net_qty\n",
        "\n",
        "    return {'quantities': net, 'rows': rows}\n",
        "\n",
        "def append_trade_row(master_df, symbol, parcel, quantity, price, exchange, date=None, sweet_value=None):\n",
        "    \"\"\"\n",
        "    Append a trade row (buy or sell) to master_df and return the new DataFrame.\n",
        "    Columns preserved: date, symbol, parcel, quantity, avg_price, exchange, SWEET_Value, token (if present)\n",
        "    \"\"\"\n",
        "    if date is None:\n",
        "        date = datetime.now().strftime(\"%d-%m-%Y\")\n",
        "    new_row = {\n",
        "        'date': date,\n",
        "        'symbol': symbol,\n",
        "        'parcel': parcel,\n",
        "        'quantity': int(math.floor(quantity)) if quantity is not None else 0,\n",
        "        'avg_price': float(price) if price is not None else 0.0,\n",
        "        'exchange': exchange if exchange is not None else '',\n",
        "        'SWEET_Value': sweet_value if sweet_value is not None else None,\n",
        "    }\n",
        "    # Attempt to preserve token column if present in master_df schema (set None by default)\n",
        "    if 'token' in master_df.columns:\n",
        "        new_row['token'] = None\n",
        "    # Append\n",
        "    appended = master_df.copy()\n",
        "    appended = appended.append(new_row, ignore_index=True)\n",
        "    logging.info(f\"append_trade_row: appended {new_row}\")\n",
        "    return appended\n",
        "\n",
        "# ---------- Main suggestion logic (refactored) ----------\n",
        "def generate_suggestions(master_portfolio_df, rules, dry_run=True):\n",
        "    \"\"\"\n",
        "    master_portfolio_df: DataFrame of master_portfolio containing P1/P2/P3/P4/S1.. entries.\n",
        "    rules: output of read_rules() (backwards-compatible dict with '_df' and '_overrides').\n",
        "    dry_run: if True -> do not persist changes; function only returns suggestions list (but still simulates calculations).\n",
        "    Returns: suggestions_df (DataFrame)\n",
        "    Each suggestion row: Select (bool), Action (BUY/SELL), Symbol, Reason, Quantity, Price, Exchange, Parcel\n",
        "    \"\"\"\n",
        "    suggestions = []\n",
        "    if master_portfolio_df is None or master_portfolio_df.empty:\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    # Load SWEET lookup table once\n",
        "    sweet_df = load_sweet_code()\n",
        "\n",
        "    # to download all the suggestions generated in rohit_audit.csv.\n",
        "    rohit_audit=[]\n",
        "\n",
        "\n",
        "    # Group by symbol\n",
        "    for symbol, group in master_portfolio_df.groupby('symbol'):\n",
        "        current_quantity=0\n",
        "        sell_qty=0\n",
        "        buy_qty=0\n",
        "        threshold_s1day=0\n",
        "        sweet_pct_change=0\n",
        "        s1day_days=0\n",
        "        p2_threshold=0\n",
        "        p3_threshold=0\n",
        "        p2_qty=0\n",
        "        p4_threshold=0\n",
        "        p2_price=0\n",
        "        p3_qty=0\n",
        "        p3_price=0\n",
        "        p4_qty=0\n",
        "\n",
        "        try:\n",
        "            # Build active parcels state\n",
        "            state = current_active_parcels(master_portfolio_df, symbol)\n",
        "            net_qty = state['quantities']  # net quantities per P1..P4 after S* adjustments\n",
        "            rows = state['rows']  # representative rows per P parcel\n",
        "            # Need a token and exchange to call get_ltp - prefer the P1 row then other P rows\n",
        "            token = None\n",
        "            exchange = None\n",
        "            for p in ['P1', 'P2', 'P3', 'P4']:\n",
        "                r = rows.get(p)\n",
        "                if r is not None:\n",
        "                    token = r.get('token', None) if 'token' in r else None\n",
        "                    exchange = r.get('exchange', None) if 'exchange' in r else None\n",
        "                    break\n",
        "\n",
        "            # Determine present top-level status: highest Pn with net > 0 (P4 > P3 > P2 > P1)\n",
        "            present_status = None\n",
        "            for p in ['P4', 'P3', 'P2', 'P1']:\n",
        "                if net_qty.get(p, 0) > 0:\n",
        "                    present_status = p\n",
        "                    break\n",
        "\n",
        "            # LTP retrieval - rely on existing get_ltp function in codebase\n",
        "            # This function signature used elsewhere: get_ltp(symbol, token, exchange)\n",
        "            try:\n",
        "                ltp = get_ltp(symbol, token, exchange) if 'get_ltp' in globals() else None\n",
        "            except Exception:\n",
        "                ltp = None\n",
        "            if ltp is None:\n",
        "                # fallback: if group has avg_price (last recorded price), use it\n",
        "                ltp = rows.get(present_status, {}).get('avg_price') if present_status else None\n",
        "            if ltp is None or ltp == 0:\n",
        "                # Can't make decisions without LTP\n",
        "                continue\n",
        "\n",
        "            # P1 must exist for almost all comparisons\n",
        "            if 'P1' not in rows or rows['P1'] is None:\n",
        "                # nothing to do if we don't have original P1\n",
        "                continue\n",
        "\n",
        "            p1_row = rows['P1']\n",
        "            p1_price = float(p1_row.get('avg_price', 0.0))\n",
        "            p1_qty_total = float(master_portfolio_df[(master_portfolio_df['symbol'] == symbol) & (master_portfolio_df['parcel'] == 'P1')]['quantity'].sum())\n",
        "            p1_date = p1_row.get('date', None)\n",
        "            p1_days = days_since(p1_date)\n",
        "\n",
        "            # current SWEET values (P1 stored SWEET if present)\n",
        "            p1_sweet = None\n",
        "            try:\n",
        "                if 'SWEET_Value' in p1_row and not pd.isna(p1_row.get('SWEET_Value')):\n",
        "                    p1_sweet = float(p1_row.get('SWEET_Value'))\n",
        "            except Exception:\n",
        "                p1_sweet = None\n",
        "            current_sweet = current_sweet_value_for_symbol(symbol, sweet_df)\n",
        "            # if current_sweet is None, fallback to p1_sweet\n",
        "            if current_sweet is None:\n",
        "                current_sweet = p1_sweet\n",
        "\n",
        "            # --- Profit branch checks for P1 (S1 and S1Day) ---\n",
        "            # S1\n",
        "            s1_rule = get_rule(rules, symbol, 'S1')\n",
        "            s1_value = float(s1_rule.get('value')) if s1_rule.get('value') is not None else None\n",
        "            s1_qty_pct = float(s1_rule.get('Quantity')) if s1_rule.get('Quantity') is not None else None\n",
        "\n",
        "            if present_status == 'P1' and s1_value is not None and s1_qty_pct is not None:\n",
        "                threshold_s1 = p1_price * (1.0 + (s1_value / 100.0))\n",
        "                if ltp >= threshold_s1:\n",
        "                    # sell S1: sell s1_qty_pct % of current_quantity\n",
        "                    # current_quantity is net of P1 minus S1\n",
        "                    current_quantity = net_qty.get('P1', 0)\n",
        "                    sell_qty = math.floor(current_quantity * (s1_qty_pct / 100.0))\n",
        "                    if sell_qty < 1 and current_quantity >= 1:\n",
        "                        sell_qty = 1\n",
        "                    if sell_qty > 0:\n",
        "                        suggestions.append({\n",
        "                            'Select': False, 'Action': 'SELL', 'Symbol': symbol,\n",
        "                            'Reason': f\"S1 Target Met: LTP {ltp:.2f} >= {threshold_s1:.2f}\",\n",
        "                            'Quantity': int(sell_qty), 'Price': round(ltp, 2),\n",
        "                            'Exchange': exchange, 'Parcel': 'S1'\n",
        "                        })\n",
        "                        rohit_audit.append({\"symbol\": symbol,\"buy_qty\":buy_qty,\"net_qty\": net_qty,\"ltp\": ltp,\"p1_price\": p1_price,\"p1_qty_total\": p1_qty_total,\"p1_date\": p1_date,\"p1_days\": p1_days,\"p1_sweet\": p1_sweet,\"current_sweet\": current_sweet,\"threshold_s1\": threshold_s1,\"current_quantity\": current_quantity,\"sell_qty\": sell_qty,\"threshold_s1day\": threshold_s1day,\"sweet_pct_change\": sweet_pct_change,\"s1day_days\": s1day_days,\"p2_threshold\": p2_threshold,\"p3_threshold\": p3_threshold,\"p2_qty\": p2_qty,\"p4_threshold\": p4_threshold,\"p2_price\": p2_price,\"p3_qty\": p3_qty,\"p3_price\": p3_price,\"p4_qty\": p4_qty          })\n",
        "\n",
        "                        # For P1 profit branch, after a sell suggestion, stop further suggestions for that symbol\n",
        "                        # to maintain priority\n",
        "                        continue\n",
        "\n",
        "                # S1Day: discount smaller gain but SWEET decreased and within DaysDiff\n",
        "                s1day_rule = get_rule(rules, symbol, 'S1Day')\n",
        "                s1day_value = s1day_rule.get('value')\n",
        "                s1day_days = s1day_rule.get('DaysDiff')\n",
        "                s1day_qty_pct = s1day_rule.get('Quantity')\n",
        "\n",
        "                sweet_diff_rule = get_rule(rules, symbol, 'SWEET_Diff')\n",
        "                sweet_diff_pct = sweet_diff_rule.get('value') if sweet_diff_rule else None\n",
        "\n",
        "                if s1day_value is not None and s1day_qty_pct is not None and s1day_days is not None and p1_days is not None:\n",
        "                    threshold_s1day = p1_price * (1.0 + (float(s1day_value) / 100.0))\n",
        "                    # SWEET change percent (current from P1)\n",
        "                    if p1_sweet is not None and current_sweet is not None:\n",
        "                        sweet_pct_change = compute_pct_change(p1_sweet, current_sweet)\n",
        "                    else:\n",
        "                        sweet_pct_change = None\n",
        "                    # Condition: LTP >= threshold_s1day and purchase age <= DaysDiff and SWEET decreased by >= SWEET_Diff\n",
        "                    if (ltp >= threshold_s1day) and (p1_days <= int(s1day_days)):\n",
        "                        if (sweet_pct_change is not None) and (sweet_diff_pct is not None):\n",
        "                            if sweet_pct_change <= -abs(float(sweet_diff_pct)):\n",
        "                                # sell\n",
        "                                current_quantity = net_qty.get('P1', 0)\n",
        "                                sell_qty = math.floor(current_quantity * (s1day_qty_pct / 100.0))\n",
        "                                if sell_qty < 1 and current_quantity >= 1:\n",
        "                                    sell_qty = 1\n",
        "                                if sell_qty > 0:\n",
        "                                    suggestions.append({\n",
        "                                        'Select': False, 'Action': 'SELL', 'Symbol': symbol,\n",
        "                                        'Reason': f\"S1Day Target Met: LTP {ltp:.2f} >= {threshold_s1day:.2f} & SWEET down {abs(sweet_pct_change):.2f}%\",\n",
        "                                        'Quantity': int(sell_qty), 'Price': round(ltp, 2),\n",
        "                                        'Exchange': exchange, 'Parcel': 'S1Day'\n",
        "                                    })\n",
        "                                    rohit_audit.append({\"symbol\": symbol,\"buy_qty\":buy_qty,\"net_qty\": net_qty,\"ltp\": ltp,\"p1_price\": p1_price,\"p1_qty_total\": p1_qty_total,\"p1_date\": p1_date,\"p1_days\": p1_days,\"p1_sweet\": p1_sweet,\"current_sweet\": current_sweet,\"threshold_s1\": threshold_s1,\"current_quantity\": current_quantity,\"sell_qty\": sell_qty,\"threshold_s1day\": threshold_s1day,\"sweet_pct_change\": sweet_pct_change,\"s1day_days\": s1day_days,\"p2_threshold\": p2_threshold,\"p3_threshold\": p3_threshold,\"p2_qty\": p2_qty,\"p4_threshold\": p4_threshold,\"p2_price\": p2_price,\"p3_qty\": p3_qty,\"p3_price\": p3_price,\"p4_qty\": p4_qty          })\n",
        "\n",
        "                                    continue\n",
        "\n",
        "            # --- Loss branch / dip buys when P1 exists (P2,P3,P4) ---\n",
        "            if p1_price is None or p1_price == 0:\n",
        "                # cannot compute buy thresholds\n",
        "                continue\n",
        "\n",
        "            # If present_status is P1 (no P2..P4 active), check P2\n",
        "            if present_status == 'P1':\n",
        "                p2_rule = get_rule(rules, symbol, 'P2')\n",
        "                if p2_rule.get('value') is not None and p2_rule.get('Quantity') is not None:\n",
        "                    p2_threshold = p1_price * (1.0 - (float(p2_rule['value']) / 100.0))\n",
        "                    if ltp <= p2_threshold:\n",
        "                        # buy P2: Quantity % of P1\n",
        "                        buy_qty = math.floor(p1_qty_total * (float(p2_rule['Quantity']) / 100.0))\n",
        "                        if buy_qty < 1:\n",
        "                            buy_qty = 1\n",
        "                        suggestions.append({\n",
        "                            'Select': False, 'Action': 'BUY', 'Symbol': symbol,\n",
        "                            'Reason': f\"P2 Target Met: LTP {ltp:.2f} <= {p2_threshold:.2f}\",\n",
        "                            'Quantity': int(buy_qty), 'Price': round(ltp, 2),\n",
        "                            'Exchange': exchange, 'Parcel': 'P2'\n",
        "                        })\n",
        "                        rohit_audit.append({\"symbol\": symbol,\"buy_qty\":buy_qty,\"net_qty\": net_qty,\"ltp\": ltp,\"p1_price\": p1_price,\"p1_qty_total\": p1_qty_total,\"p1_date\": p1_date,\"p1_days\": p1_days,\"p1_sweet\": p1_sweet,\"current_sweet\": current_sweet,\"threshold_s1\": threshold_s1,\"current_quantity\": current_quantity,\"sell_qty\": sell_qty,\"threshold_s1day\": threshold_s1day,\"sweet_pct_change\": sweet_pct_change,\"s1day_days\": s1day_days,\"p2_threshold\": p2_threshold,\"p3_threshold\": p3_threshold,\"p2_qty\": p2_qty,\"p4_threshold\": p4_threshold,\"p2_price\": p2_price,\"p3_qty\": p3_qty,\"p3_price\": p3_price,\"p4_qty\": p4_qty          })\n",
        "\n",
        "                        # Only first applied buy per symbol\n",
        "                        continue\n",
        "\n",
        "            # If present_status == P2 (P2 exists), check for P3 or S2\n",
        "            if present_status == 'P2':\n",
        "                p3_rule = get_rule(rules, symbol, 'P3')\n",
        "                if p3_rule.get('value') is not None and p3_rule.get('Quantity') is not None:\n",
        "                    p3_threshold = p1_price * (1.0 - (float(p3_rule['value']) / 100.0))\n",
        "                    if ltp <= p3_threshold and 'P3' not in rows:\n",
        "                        buy_qty = math.floor(p1_qty_total * (float(p3_rule['Quantity']) / 100.0))\n",
        "                        if buy_qty < 1:\n",
        "                            buy_qty = 1\n",
        "                        suggestions.append({\n",
        "                            'Select': False, 'Action': 'BUY', 'Symbol': symbol,\n",
        "                            'Reason': f\"P3 Target Met: LTP {ltp:.2f} <= {p3_threshold:.2f}\",\n",
        "                            'Quantity': int(buy_qty), 'Price': round(ltp, 2),\n",
        "                            'Exchange': exchange, 'Parcel': 'P3'\n",
        "                        })\n",
        "                        rohit_audit.append({\"symbol\": symbol,\"buy_qty\":buy_qty,\"net_qty\": net_qty,\"ltp\": ltp,\"p1_price\": p1_price,\"p1_qty_total\": p1_qty_total,\"p1_date\": p1_date,\"p1_days\": p1_days,\"p1_sweet\": p1_sweet,\"current_sweet\": current_sweet,\"threshold_s1\": threshold_s1,\"current_quantity\": current_quantity,\"sell_qty\": sell_qty,\"threshold_s1day\": threshold_s1day,\"sweet_pct_change\": sweet_pct_change,\"s1day_days\": s1day_days,\"p2_threshold\": p2_threshold,\"p3_threshold\": p3_threshold,\"p2_qty\": p2_qty,\"p4_threshold\": p4_threshold,\"p2_price\": p2_price,\"p3_qty\": p3_qty,\"p3_price\": p3_price,\"p4_qty\": p4_qty          })\n",
        "\n",
        "                        continue\n",
        "                # S2: if LTP >= P1_price then sell all P2\n",
        "                if ltp >= p1_price:\n",
        "                    p2_qty = net_qty.get('P2', 0)\n",
        "                    if p2_qty > 0:\n",
        "                        suggestions.append({\n",
        "                            'Select': False, 'Action': 'SELL', 'Symbol': symbol,\n",
        "                            'Reason': f\"S2 Condition Met: LTP {ltp:.2f} >= P1_price {p1_price:.2f}\",\n",
        "                            'Quantity': int(math.floor(p2_qty)), 'Price': round(ltp, 2),\n",
        "                            'Exchange': exchange, 'Parcel': 'S2'\n",
        "                        })\n",
        "                        rohit_audit.append({\"symbol\": symbol,\"buy_qty\":buy_qty,\"net_qty\": net_qty,\"ltp\": ltp,\"p1_price\": p1_price,\"p1_qty_total\": p1_qty_total,\"p1_date\": p1_date,\"p1_days\": p1_days,\"p1_sweet\": p1_sweet,\"current_sweet\": current_sweet,\"threshold_s1\": threshold_s1,\"current_quantity\": current_quantity,\"sell_qty\": sell_qty,\"threshold_s1day\": threshold_s1day,\"sweet_pct_change\": sweet_pct_change,\"s1day_days\": s1day_days,\"p2_threshold\": p2_threshold,\"p3_threshold\": p3_threshold,\"p2_qty\": p2_qty,\"p4_threshold\": p4_threshold,\"p2_price\": p2_price,\"p3_qty\": p3_qty,\"p3_price\": p3_price,\"p4_qty\": p4_qty          })\n",
        "\n",
        "                        continue\n",
        "\n",
        "            # If present_status == P3, check for P4 or S3\n",
        "            if present_status == 'P3':\n",
        "                p4_rule = get_rule(rules, symbol, 'P4')\n",
        "                if p4_rule.get('value') is not None and p4_rule.get('Quantity') is not None:\n",
        "                    p4_threshold = p1_price * (1.0 - (float(p4_rule['value']) / 100.0))\n",
        "                    if ltp <= p4_threshold and 'P4' not in rows:\n",
        "                        buy_qty = math.floor(p1_qty_total * (float(p4_rule['Quantity']) / 100.0))\n",
        "                        if buy_qty < 1:\n",
        "                            buy_qty = 1\n",
        "                        suggestions.append({\n",
        "                            'Select': False, 'Action': 'BUY', 'Symbol': symbol,\n",
        "                            'Reason': f\"P4 Target Met: LTP {ltp:.2f} <= {p4_threshold:.2f}\",\n",
        "                            'Quantity': int(buy_qty), 'Price': round(ltp, 2),\n",
        "                            'Exchange': exchange, 'Parcel': 'P4'\n",
        "                        })\n",
        "                        rohit_audit.append({\"symbol\": symbol,\"buy_qty\":buy_qty,\"net_qty\": net_qty,\"ltp\": ltp,\"p1_price\": p1_price,\"p1_qty_total\": p1_qty_total,\"p1_date\": p1_date,\"p1_days\": p1_days,\"p1_sweet\": p1_sweet,\"current_sweet\": current_sweet,\"threshold_s1\": threshold_s1,\"current_quantity\": current_quantity,\"sell_qty\": sell_qty,\"threshold_s1day\": threshold_s1day,\"sweet_pct_change\": sweet_pct_change,\"s1day_days\": s1day_days,\"p2_threshold\": p2_threshold,\"p3_threshold\": p3_threshold,\"p2_qty\": p2_qty,\"p4_threshold\": p4_threshold,\"p2_price\": p2_price,\"p3_qty\": p3_qty,\"p3_price\": p3_price,\"p4_qty\": p4_qty          })\n",
        "\n",
        "                        continue\n",
        "                # S3: if LTP >= P2_price then sell all P3\n",
        "                p2_row = rows.get('P2', None)\n",
        "                if p2_row is not None:\n",
        "                    p2_price = float(p2_row.get('avg_price', 0.0))\n",
        "                    if p2_price and ltp >= p2_price:\n",
        "                        p3_qty = net_qty.get('P3', 0)\n",
        "                        if p3_qty > 0:\n",
        "                            suggestions.append({\n",
        "                                'Select': False, 'Action': 'SELL', 'Symbol': symbol,\n",
        "                                'Reason': f\"S3 Condition Met: LTP {ltp:.2f} >= P2_price {p2_price:.2f}\",\n",
        "                                'Quantity': int(math.floor(p3_qty)), 'Price': round(ltp, 2),\n",
        "                                'Exchange': exchange, 'Parcel': 'S3'\n",
        "                            })\n",
        "                            rohit_audit.append({\"symbol\": symbol,\"buy_qty\":buy_qty,\"net_qty\": net_qty,\"ltp\": ltp,\"p1_price\": p1_price,\"p1_qty_total\": p1_qty_total,\"p1_date\": p1_date,\"p1_days\": p1_days,\"p1_sweet\": p1_sweet,\"current_sweet\": current_sweet,\"threshold_s1\": threshold_s1,\"current_quantity\": current_quantity,\"sell_qty\": sell_qty,\"threshold_s1day\": threshold_s1day,\"sweet_pct_change\": sweet_pct_change,\"s1day_days\": s1day_days,\"p2_threshold\": p2_threshold,\"p3_threshold\": p3_threshold,\"p2_qty\": p2_qty,\"p4_threshold\": p4_threshold,\"p2_price\": p2_price,\"p3_qty\": p3_qty,\"p3_price\": p3_price,\"p4_qty\": p4_qty          })\n",
        "\n",
        "                            continue\n",
        "\n",
        "            # If present_status == P4, check S4 condition\n",
        "            if present_status == 'P4':\n",
        "                p3_row = rows.get('P3', None)\n",
        "                if p3_row is not None:\n",
        "                    p3_price = float(p3_row.get('avg_price', 0.0))\n",
        "                    if p3_price and ltp >= p3_price:\n",
        "                        p4_qty = net_qty.get('P4', 0)\n",
        "                        if p4_qty > 0:\n",
        "                            suggestions.append({\n",
        "                                'Select': False, 'Action': 'SELL', 'Symbol': symbol,\n",
        "                                'Reason': f\"S4 Condition Met: LTP {ltp:.2f} >= P3_price {p3_price:.2f}\",\n",
        "                                'Quantity': int(math.floor(p4_qty)), 'Price': round(ltp, 2),\n",
        "                                'Exchange': exchange, 'Parcel': 'S4'\n",
        "                            })\n",
        "                            rohit_audit.append({\"symbol\": symbol,\"buy_qty\":buy_qty,\"net_qty\": net_qty,\"ltp\": ltp,\"p1_price\": p1_price,\"p1_qty_total\": p1_qty_total,\"p1_date\": p1_date,\"p1_days\": p1_days,\"p1_sweet\": p1_sweet,\"current_sweet\": current_sweet,\"threshold_s1\": threshold_s1,\"current_quantity\": current_quantity,\"sell_qty\": sell_qty,\"threshold_s1day\": threshold_s1day,\"sweet_pct_change\": sweet_pct_change,\"s1day_days\": s1day_days,\"p2_threshold\": p2_threshold,\"p3_threshold\": p3_threshold,\"p2_qty\": p2_qty,\"p4_threshold\": p4_threshold,\"p2_price\": p2_price,\"p3_qty\": p3_qty,\"p3_price\": p3_price,\"p4_qty\": p4_qty          })\n",
        "\n",
        "                            continue\n",
        "                # If LTP declines further from P4 price: do nothing per rules\n",
        "\n",
        "            rohit_audit.append({\"symbol\": symbol,\"buy_qty\":buy_qty,\"net_qty\": net_qty,\"ltp\": ltp,\"p1_price\": p1_price,\"p1_qty_total\": p1_qty_total,\"p1_date\": p1_date,\"p1_days\": p1_days,\"p1_sweet\": p1_sweet,\"current_sweet\": current_sweet,\"threshold_s1\": threshold_s1,\"current_quantity\": current_quantity,\"sell_qty\": sell_qty,\"threshold_s1day\": threshold_s1day,\"sweet_pct_change\": sweet_pct_change,\"s1day_days\": s1day_days,\"p2_threshold\": p2_threshold,\"p3_threshold\": p3_threshold,\"p2_qty\": p2_qty,\"p4_threshold\": p4_threshold,\"p2_price\": p2_price,\"p3_qty\": p3_qty,\"p3_price\": p3_price,\"p4_qty\": p4_qty          })\n",
        "\n",
        "        except Exception as e:\n",
        "            logging.exception(f\"Error processing symbol {symbol}: {e}\")\n",
        "            continue\n",
        "\n",
        "    rohit_audit_df = pd.DataFrame(rohit_audit)\n",
        "    rohit_audit_df.to_csv(\"downloadrohit/rohit_audit_csv.csv\",index=False)\n",
        "\n",
        "    # convert suggestions to DataFrame and return\n",
        "    suggestions_df = pd.DataFrame(suggestions)\n",
        "    # default Select to False column if missing\n",
        "    if not suggestions_df.empty and 'Select' not in suggestions_df.columns:\n",
        "        suggestions_df['Select'] = False\n",
        "    return suggestions_df\n",
        "\n",
        "# ---------- Simple update_and_archive_portfolio wrapper ----------\n",
        "def update_and_archive_portfolio(portfolio_df, trades_df, rules, dry_run=True):\n",
        "    \"\"\"\n",
        "    This function orchestrates reading current master_portfolio (portfolio_df is expected to be the master_portfolio),\n",
        "    generating suggestions, and optionally appending trade rows for simulated executed suggestions (if not dry_run).\n",
        "    Returns: (updated_master_portfolio_df, status_string)\n",
        "    NOTE: This wrapper intentionally keeps persistence calls outside; original callers manage saving to CSV.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        if portfolio_df is None:\n",
        "            portfolio_df = pd.DataFrame()\n",
        "        suggestions_df = generate_suggestions(portfolio_df, rules, dry_run=True)\n",
        "\n",
        "        status_msgs = []\n",
        "        status_msgs.append(f\"Scanned {len(portfolio_df['symbol'].unique()) if not portfolio_df.empty else 0} symbols.\")\n",
        "        status_msgs.append(f\"Generated {len(suggestions_df)} suggestions (dry_run={dry_run}).\")\n",
        "\n",
        "        # If not dry_run, simulate executing suggestions by appending to master_portfolio\n",
        "        updated = portfolio_df.copy()\n",
        "        if (not dry_run) and (not suggestions_df.empty):\n",
        "            for _, s in suggestions_df.iterrows():\n",
        "                # map Action/Parcel into appended row\n",
        "                parcel = s.get('Parcel')\n",
        "                qty = s.get('Quantity', 0)\n",
        "                price = s.get('Price', None)\n",
        "                exch = s.get('Exchange', None)\n",
        "                symbol = s.get('Symbol')\n",
        "                # compute sweet_value at time of trade\n",
        "                sv = sweet_value_for_symbol(symbol, updated)\n",
        "                updated = append_trade_row(updated, symbol, parcel, qty, price, exch, date=datetime.now().strftime(\"%d-%m-%Y\"), sweet_value=sv)\n",
        "            status_msgs.append(f\"Appended {len(suggestions_df)} trade rows to master_portfolio.\")\n",
        "        return updated, \"\\n\".join(status_msgs)\n",
        "    except Exception as e:\n",
        "        logging.exception(f\"update_and_archive_portfolio error: {e}\")\n",
        "        return portfolio_df, f\"Error: {e}\"\n",
        "\n",
        "def on_analyze_and_suggest_click():\n",
        "    rules = read_rules()\n",
        "    portfolio_df = load_portfolio_from_csv()\n",
        "    trades_df = get_trade_book()\n",
        "\n",
        "    updated_portfolio_df, status = update_and_archive_portfolio(portfolio_df, trades_df, rules)\n",
        "\n",
        "    if not updated_portfolio_df.empty:\n",
        "        save_portfolio_to_csv(updated_portfolio_df)\n",
        "\n",
        "    suggestions_df = generate_suggestions(updated_portfolio_df, rules)\n",
        "    status += f\"\\nFound {len(suggestions_df)} new suggestions.\"\n",
        "    return status, suggestions_df, gr.update(value=updated_portfolio_df)\n",
        "\n",
        "\n",
        "def execute_selected_trades(suggestions_df):\n",
        "    results = []\n",
        "    if not isinstance(suggestions_df, pd.DataFrame) or suggestions_df.empty:\n",
        "        return \"No suggestions to execute.\"\n",
        "\n",
        "    selected_trades = suggestions_df[suggestions_df['Select'] == True]\n",
        "    if selected_trades.empty:\n",
        "        return \"No trades were selected.\"\n",
        "\n",
        "    for _, row in selected_trades.iterrows():\n",
        "        result = place_order(\n",
        "            tradingsymbol=row['Symbol'], transactiontype=row['Action'],\n",
        "            quantity=row['Quantity'], ordertype=\"LIMIT\",\n",
        "            price=row['Price'], exchange=row['Exchange'], producttype=\"DELIVERY\"\n",
        "        )\n",
        "        results.append(result)\n",
        "        time.sleep(SLEEP*5)\n",
        "\n",
        "    return \"\\n\".join(results)"
      ],
      "metadata": {
        "id": "5TXnQaNmdZMH"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **common for automated and manual Buy and Sell**"
      ],
      "metadata": {
        "id": "Utr8Zmwwe3k-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def place_order(tradingsymbol, transactiontype, quantity, ordertype, price, exchange, producttype):\n",
        "    result_message = \"\"\n",
        "    if not smartApi:\n",
        "        return \"Order not placed: API not connected.\"\n",
        "\n",
        "    lookup_map = nse_symbol_token_map if exchange == 'NSE' else bse_symbol_token_map\n",
        "    token = lookup_map.get(tradingsymbol)\n",
        "    if not token:\n",
        "        return f\"â—ï¸ Could not find token for {tradingsymbol} on {exchange}.\"\n",
        "\n",
        "    try:\n",
        "        order_params = {\n",
        "            \"variety\": \"NORMAL\", \"tradingsymbol\": tradingsymbol,\n",
        "            \"symboltoken\": str(token), \"transactiontype\": transactiontype,\n",
        "            \"exchange\": exchange, \"ordertype\": ordertype,\n",
        "            \"producttype\": producttype, \"duration\": \"DAY\",\n",
        "            \"quantity\": str(quantity), \"squareoff\": \"0\", \"stoploss\": \"0\"\n",
        "        }\n",
        "        if ordertype == \"LIMIT\":\n",
        "            if price is None or price <= 0:\n",
        "                return \"â—ï¸ Price must be provided for LIMIT orders.\"\n",
        "            order_params[\"price\"] = str(round(price, 2))\n",
        "\n",
        "        order_response = smartApi.placeOrderFullResponse(order_params)\n",
        "\n",
        "        if isinstance(order_response, dict):\n",
        "            order_id = None\n",
        "            if 'data' in order_response and isinstance(order_response['data'], dict):\n",
        "                order_id = order_response['data'].get('orderid')\n",
        "            if not order_id:\n",
        "                order_id = order_response.get('orderid')\n",
        "\n",
        "            if order_id:\n",
        "                result_message = f\"âœ… {transactiontype.upper()} Order placed for {tradingsymbol}! ID: {order_id}\"\n",
        "            else:\n",
        "                result_message = f\"â—ï¸ {transactiontype.upper()} Failed for {tradingsymbol}: {order_response.get('message', 'Unknown API error')}\"\n",
        "        else:\n",
        "            result_message = f\"â—ï¸ {transactiontype.upper()} Failed for {tradingsymbol}: {str(order_response)}\"\n",
        "    except Exception as e:\n",
        "        result_message = f\"â—ï¸ Error placing {transactiontype.upper()} order for {tradingsymbol}: {e}\"\n",
        "\n",
        "    logging.info(result_message)\n",
        "    return result_message"
      ],
      "metadata": {
        "id": "Qzen-hU4eqru"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Watchlist**"
      ],
      "metadata": {
        "id": "8u0jTDVzjhKF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "CSV_PATH = industry\n",
        "SYMBOL_COLUMN_NAME = \"Name\"\n",
        "NEW_COLUMNS = {\n",
        "    \"Remark_AI\": \"\", \"Remark_1\": \"\", \"Remark_2\": \"\", \"Remark_3\": \"\", \"Remark_4\": \"\",\n",
        "    \"watchlist_flag\": False,\n",
        "}\n",
        "\n",
        "# --- Core Data Functions ---\n",
        "\n",
        "def load_or_initialize_data():\n",
        "    \"\"\"\n",
        "    Loads data and adds necessary columns. Raises an error if the symbol column is not found.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        df = pd.read_csv(CSV_PATH)\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Warning: '{CSV_PATH}' not found. Creating a dummy file for demonstration.\", file=sys.stderr)\n",
        "        df = pd.DataFrame({\n",
        "            SYMBOL_COLUMN_NAME: ['SAMPLE1', 'SAMPLE2'], 'Industry': ['Tech', 'Finance'],\n",
        "        })\n",
        "\n",
        "    if SYMBOL_COLUMN_NAME not in df.columns:\n",
        "        raise ValueError(\n",
        "            f\"Fatal Error: Column '{SYMBOL_COLUMN_NAME}' not found in '{CSV_PATH}'. \"\n",
        "            f\"Please check the SYMBOL_COLUMN_NAME variable in the script.\"\n",
        "        )\n",
        "\n",
        "    updated = False\n",
        "    for col, default_value in NEW_COLUMNS.items():\n",
        "        if col not in df.columns:\n",
        "            df[col] = default_value\n",
        "            updated = True\n",
        "\n",
        "    if 'watchlist_flag' not in df.columns or df['watchlist_flag'].dtype != bool:\n",
        "        df['watchlist_flag'] = df.get('watchlist_flag', pd.Series(False)).fillna(False).astype(bool)\n",
        "        updated = True\n",
        "\n",
        "    if updated:\n",
        "        df.to_csv(CSV_PATH, index=False)\n",
        "\n",
        "    return df\n",
        "\n",
        "def save_data_to_csv(dataframe):\n",
        "    \"\"\"Saves the DataFrame to the CSV file.\"\"\"\n",
        "    dataframe.to_csv(CSV_PATH, index=False)\n",
        "    watchlist_df = dataframe[dataframe[\"watchlist_flag\"] == True]\n",
        "    return (\n",
        "        f\"âœ… Success! Data saved to {CSV_PATH} at {datetime.now().strftime('%H:%M:%S')}\",\n",
        "        watchlist_df\n",
        "    )\n",
        "\n",
        "# --- Gradio Interaction Functions ---\n",
        "\n",
        "def get_initial_views():\n",
        "    \"\"\"Loads data and provides initial UI component values.\"\"\"\n",
        "    df = load_or_initialize_data()\n",
        "    symbol_list = df[SYMBOL_COLUMN_NAME].tolist()\n",
        "    watchlist_df = df[df[\"watchlist_flag\"] == True]\n",
        "    return df, gr.Dropdown(choices=symbol_list), pd.DataFrame(), watchlist_df\n",
        "\n",
        "def display_selected_symbol(df_state, selected_symbol):\n",
        "    \"\"\"Filters the dataframe to show only the selected symbol's data.\"\"\"\n",
        "    if not selected_symbol:\n",
        "        return pd.DataFrame()\n",
        "    return df_state[df_state[SYMBOL_COLUMN_NAME] == selected_symbol]\n",
        "\n",
        "def update_data_on_edit(df_state, selected_symbol, updated_detail_df):\n",
        "    \"\"\"\n",
        "    Handles inline edits by taking the entire updated detail DataFrame as input,\n",
        "    which bypasses the buggy gr.EditData event object.\n",
        "    \"\"\"\n",
        "    if not selected_symbol or updated_detail_df is None or updated_detail_df.empty:\n",
        "        return df_state, df_state[df_state[\"watchlist_flag\"] == True]\n",
        "\n",
        "    updated_row = updated_detail_df.iloc[0]\n",
        "    original_index = df_state[df_state[SYMBOL_COLUMN_NAME] == selected_symbol].index[0]\n",
        "\n",
        "    # Update the master dataframe with all values from the edited row\n",
        "    for col_name, new_value in updated_row.items():\n",
        "        if col_name in df_state.columns:\n",
        "            df_state.loc[original_index, col_name] = new_value\n",
        "\n",
        "    watchlist_df = df_state[df_state[\"watchlist_flag\"] == True]\n",
        "    return df_state, watchlist_df\n",
        "\n",
        "def generate_ai_remark(df_state, selected_symbol):\n",
        "    \"\"\"Generates a sample AI remark for the selected symbol.\"\"\"\n",
        "    if not selected_symbol:\n",
        "        gr.Warning(\"Please select a company from the dropdown first.\")\n",
        "        return df_state, pd.DataFrame(), df_state[df_state[\"watchlist_flag\"] == True]\n",
        "\n",
        "    timestamp = datetime.now().strftime('%Y-%m-%d %H:%M')\n",
        "    remark = f\"AI Note for {selected_symbol}: Monitored on {timestamp}.\"\n",
        "\n",
        "    original_index = df_state[df_state[SYMBOL_COLUMN_NAME] == selected_symbol].index[0]\n",
        "    df_state.loc[original_index, 'Remark_AI'] = remark\n",
        "\n",
        "    gr.Info(f\"AI remark generated for {selected_symbol}.\")\n",
        "    updated_view = df_state[df_state[SYMBOL_COLUMN_NAME] == selected_symbol]\n",
        "    watchlist_df = df_state[df_state[\"watchlist_flag\"] == True]\n",
        "    return df_state, updated_view, watchlist_df\n",
        "\n",
        "def reload_watchlist(_):\n",
        "    \"\"\"Reloads the watchlist fresh from CSV (ignores stale memory).\"\"\"\n",
        "    df = load_or_initialize_data()\n",
        "    watchlist = df[df[\"watchlist_flag\"] == True]\n",
        "    if watchlist.empty:\n",
        "        return pd.DataFrame({\"Message\": [\"â­ No companies in watchlist yet.\"]})\n",
        "    return watchlist\n",
        "\n",
        "def save_watchlist_changes(edited_watchlist):\n",
        "    \"\"\"Update master data with edited watchlist and persist to CSV.\"\"\"\n",
        "    df = load_or_initialize_data()\n",
        "\n",
        "    # ensure both are DataFrames\n",
        "    edited_watchlist = pd.DataFrame(edited_watchlist)\n",
        "\n",
        "    for _, row in edited_watchlist.iterrows():\n",
        "        symbol = row[SYMBOL_COLUMN_NAME]\n",
        "\n",
        "        if symbol in df[SYMBOL_COLUMN_NAME].values:\n",
        "            for col in df.columns:\n",
        "                if col in row.index:\n",
        "                    try:\n",
        "                        # Cast row[col] to the dtype of df[col]\n",
        "                        casted_value = row[col]\n",
        "                        target_dtype = df[col].dtype\n",
        "\n",
        "                        if pd.isna(casted_value):\n",
        "                            df.loc[df[SYMBOL_COLUMN_NAME] == symbol, col] = pd.NA\n",
        "                        else:\n",
        "                            if target_dtype == \"bool\":\n",
        "                                casted_value = str(casted_value).strip().lower() in [\"true\", \"1\", \"yes\"]\n",
        "                            else:\n",
        "                                casted_value = target_dtype.type(casted_value)\n",
        "\n",
        "                            df.loc[df[SYMBOL_COLUMN_NAME] == symbol, col] = casted_value\n",
        "\n",
        "                    except Exception as e:\n",
        "                        print(f\"âš ï¸ Skipped casting error for col={col}, value={row[col]}: {e}\")\n",
        "\n",
        "    # persist to CSV\n",
        "    df.to_csv(CSV_PATH, index=False)\n",
        "\n",
        "    return \"âœ… Watchlist changes saved!\", df[df[\"watchlist_flag\"] == True]\n"
      ],
      "metadata": {
        "id": "j-WeaginjnDB"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Gradio UI**"
      ],
      "metadata": {
        "id": "icSYjlpwSOSa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "load_script_masters()\n",
        "with gr.Blocks(theme=gr.themes.Glass(), title=\"SWEET\") as iface:\n",
        "    gr.Markdown(\"## ðŸ“ˆ SWEET Pathway\")\n",
        "    gr.Markdown(\"### Smart Wealth Enhancement & Equity Trading Algo\")\n",
        "\n",
        "    with gr.Tabs() as tabs:\n",
        "        with gr.TabItem(\"Dashboard\"):\n",
        "            gr.Markdown(\"## ðŸ“Š Dashboard\")\n",
        "\n",
        "            btn = gr.Button(\"Fetch & Analyze Portfolio\", variant=\"primary\")\n",
        "\n",
        "            summary_table = gr.Dataframe(label=\"Summary Metrics\", interactive=False, wrap=True)\n",
        "\n",
        "            with gr.Row():\n",
        "                chart1 = gr.Plot(label=\".\")\n",
        "                chart2 = gr.Plot(label=\".\")\n",
        "\n",
        "            chart3 = gr.Plot(label=\".\")\n",
        "\n",
        "            gr.Markdown(\"### Holdings Details\")\n",
        "            portfolio_table = gr.Dataframe(label=\"Portfolio Holdings\", interactive=False)\n",
        "\n",
        "            # New component to handle the CSV download\n",
        "            download_file = gr.File(label=\"Download Portfolio CSV\", interactive=False)\n",
        "\n",
        "            # Update the button's click action to populate all outputs\n",
        "            btn.click(\n",
        "                fn=analyze_portfolio,\n",
        "                outputs=[portfolio_table, summary_table, chart1, chart2, chart3, download_file]\n",
        "            )\n",
        "\n",
        "\n",
        "        with gr.TabItem(\"Automated Trading\"):\n",
        "            gr.Markdown(\"## Rule-Based Trading Suggestions\")\n",
        "            auto_trade_status = gr.Textbox(label=\"Status\", interactive=False)\n",
        "            analyze_suggest_btn = gr.Button(\"ðŸ”„ Analyze & Generate Suggestions\", variant=\"primary\")\n",
        "            suggestions_df_output = gr.DataFrame(label=\"Trade Suggestions\",\n",
        "                                                 interactive=True,\n",
        "                                                 datatype=['bool', 'str', 'str', 'str', 'number', 'number', 'str', 'str'])\n",
        "            execute_trades_btn = gr.Button(\"ðŸš€ Execute Selected Trades\")\n",
        "            execution_status = gr.Textbox(label=\"Execution Results\", interactive=False, lines=5)\n",
        "\n",
        "        with gr.TabItem(\"Manual Trading\"):\n",
        "            gr.Markdown(\"## Manual Order Placement\")\n",
        "            manual_trade_status = gr.Textbox(label=\"Order Status\", interactive=False)\n",
        "\n",
        "            with gr.Row():\n",
        "                with gr.Group():\n",
        "                    gr.Markdown(\"### ðŸ”¼ Manual Buy\")\n",
        "                    buy_exchange = gr.Radio(label=\"Exchange\", choices=[\"NSE\", \"BSE\"], value=\"NSE\")\n",
        "                    buy_symbol = gr.Dropdown(choices=nse_symbols, label=\"Symbol\")\n",
        "                    buy_qty = gr.Number(label=\"Quantity\", value=1, minimum=1)\n",
        "                    buy_product = gr.Radio(label=\"Product\", choices=[\"DELIVERY\"], value=\"DELIVERY\")\n",
        "                    buy_order_type = gr.Radio(label=\"Order Type\", choices=[\"MARKET\", \"LIMIT\"], value=\"MARKET\")\n",
        "                    buy_price = gr.Number(label=\"Price\", value=0, visible=False)\n",
        "                    place_buy_order_btn = gr.Button(\"Place Buy Order\", variant=\"primary\")\n",
        "\n",
        "                with gr.Group():\n",
        "                    gr.Markdown(\"### ðŸ”½ Manual Sell\")\n",
        "                    sell_exchange = gr.Radio(label=\"Exchange\", choices=[\"NSE\", \"BSE\"], value=\"NSE\")\n",
        "                    sell_symbol = gr.Dropdown(choices=nse_symbols, label=\"Symbol\")\n",
        "                    sell_qty = gr.Number(label=\"Quantity\", value=1, minimum=1)\n",
        "                    sell_product = gr.Radio(label=\"Product\", choices=[\"DELIVERY\"], value=\"DELIVERY\")\n",
        "                    sell_order_type = gr.Radio(label=\"Order Type\", choices=[\"MARKET\", \"LIMIT\"], value=\"MARKET\")\n",
        "                    sell_price = gr.Number(label=\"Price\", value=0, visible=False)\n",
        "                    place_sell_order_btn = gr.Button(\"Place Sell Order\", variant=\"stop\")\n",
        "\n",
        "\n",
        "\n",
        "        with gr.TabItem(\"Trade Book\"):\n",
        "            gr.Markdown(\"## Trade Book and Order Book\")\n",
        "\n",
        "            gr.Markdown(\"## ðŸ“‘ Your Order Book\")\n",
        "            order_output = gr.DataFrame(label=\"Order Book\", interactive=False)\n",
        "            get_orders_btn = gr.Button(\"Get Order Book\")\n",
        "            get_orders_btn.click(get_order_book, outputs=order_output)\n",
        "\n",
        "            gr.Markdown(\"### Trade History\")\n",
        "            gr.Markdown(\"## ðŸ“‘ Your Trade Book\")\n",
        "            trade_output = gr.DataFrame(label=\"Trade Book\", interactive=False)\n",
        "            get_trades_btn = gr.Button(\"Get Trade Book\")\n",
        "            get_trades_btn.click(get_trade_book, outputs=trade_output)\n",
        "\n",
        "\n",
        "        with gr.TabItem(\"Master Portfolio\"):\n",
        "            gr.Markdown(\"## Master Portfolio View, Edit, and Sync\")\n",
        "            status_mp = gr.Textbox(label=\"Status\", interactive=False)\n",
        "\n",
        "            load_mp_btn = gr.Button(\"ðŸ“‚ Load Master Portfolio\")\n",
        "\n",
        "            portfolio_df_editor = gr.DataFrame(\n",
        "                label=\"Portfolio Editor\",\n",
        "                interactive=True,\n",
        "                datatype=['str', 'str', 'str', 'number', 'number', 'str', 'number', 'bool']\n",
        "            )\n",
        "\n",
        "            with gr.Column():\n",
        "              save_mp_btn = gr.Button(\"ðŸ’¾ Save Changes (for manual edits)\", variant=\"primary\")\n",
        "              with gr.Row():\n",
        "                add_row_btn = gr.Button(\"âž• Add New Row\")\n",
        "                delete_row_btn = gr.Button(\"âŒ Apply Deletions & Save\")\n",
        "\n",
        "            gr.Markdown(\"---\")\n",
        "            gr.Markdown(\"### ðŸ”„ Sync & Verify\")\n",
        "            with gr.Row():\n",
        "              preview_trades_btn = gr.Button(\"ðŸ“¥ Preview New Trades from Tradebook\")\n",
        "              confirm_add_btn = gr.Button(\"Confirm and Add to Portfolio Editor\", variant=\"secondary\")\n",
        "\n",
        "            gr.Markdown(\"### Preview of New Entries from Trade Book\")\n",
        "            trade_preview_df = gr.DataFrame(label=\"Trade Preview\")\n",
        "\n",
        "            mismatch_check_btn = gr.Button(\"ðŸ” Check Mismatches with Angel One\")\n",
        "            gr.Markdown(\"### Mismatch Report\")\n",
        "            mismatch_output = gr.Markdown()\n",
        "\n",
        "            with gr.Accordion(\"ðŸ—„ï¸ Archive\", open=True):\n",
        "              with gr.Row():\n",
        "                archive_button = gr.Button(\"ðŸš€ Run Archiving Process\", variant=\"primary\")\n",
        "                view_archive_btn = gr.Button(\"View Archived Trades\")\n",
        "              archive_df_display = gr.DataFrame(label=\"Archived Trades\")\n",
        "\n",
        "        with gr.TabItem(\"Historical Data Downloader\"):\n",
        "            gr.Markdown(\"### Get Historical Candle Data for Symbols\")\n",
        "            with gr.Row():\n",
        "                exchange_input = gr.Radio([\"NSE\", \"BSE\"], label=\"Exchange\", value=\"NSE\")\n",
        "\n",
        "            input_method_radio = gr.Radio([\"Manual Selection\", \"Upload Excel File\"], label=\"Select Input Method\", value=\"Manual Selection\")\n",
        "\n",
        "            with gr.Column(visible=True) as manual_input_column:\n",
        "                symbol_input = gr.Dropdown(choices=nse_symbols, label=\"Symbols\", multiselect=True)\n",
        "                select_all_btn = gr.Button(\"Select All Symbols\")\n",
        "\n",
        "            with gr.Column(visible=False) as upload_input_column:\n",
        "                excel_file_input = gr.File(label=\"Upload Excel File (with 'Token Symbol' column)\", file_types=[\".xls\", \".xlsx\"])\n",
        "\n",
        "            gr.Markdown(\"---\")\n",
        "\n",
        "            with gr.Row():\n",
        "                interval_input = gr.Radio([\"ONE_DAY\", \"FIVE_MINUTE\"], value=\"ONE_DAY\")\n",
        "            with gr.Row():\n",
        "                from_date_input = gr.Textbox(value=default_from_date, label=\"From Date\")\n",
        "                to_date_input = gr.Textbox(value=default_to_date, label=\"To Date\")\n",
        "\n",
        "            with gr.Row():\n",
        "                get_btn = gr.Button(\"Get Data\")\n",
        "\n",
        "            output_table = gr.Dataframe()\n",
        "            download_file = gr.File(label=\"Download Historical Data\")\n",
        "            status_box_data = gr.Textbox(label=\"Status\", interactive=False)\n",
        "\n",
        "            exchange_input.change(update_symbol_dropdown, inputs=exchange_input, outputs=symbol_input)\n",
        "            select_all_btn.click(select_all_symbols, inputs=exchange_input, outputs=symbol_input)\n",
        "\n",
        "            input_method_radio.change(\n",
        "                fn=toggle_input,\n",
        "                inputs=input_method_radio,\n",
        "                outputs=[manual_input_column, symbol_input, upload_input_column, select_all_btn]\n",
        "            )\n",
        "\n",
        "            get_btn.click(get_data_and_prepare_download,\n",
        "                        inputs=[\n",
        "                            input_method_radio,\n",
        "                            symbol_input,\n",
        "                            excel_file_input,\n",
        "                            exchange_input,\n",
        "                            interval_input,\n",
        "                            from_date_input,\n",
        "                            to_date_input\n",
        "                        ],\n",
        "                        outputs=[output_table, download_file, status_box_data])\n",
        "\n",
        "            gr.Markdown(\"### Download the Full Scrip Master List\")\n",
        "            gr.Markdown(\"This list is useful for finding the correct symbols and token numbers.\")\n",
        "            with gr.Row():\n",
        "                exchange_master = gr.Radio([\"NSE\", \"BSE\"], label=\"Select Exchange for Master List\", value=\"NSE\")\n",
        "                download_scrip_master_btn = gr.Button(\"Download Script Master as CSV\")\n",
        "                scrip_master_file = gr.File(label=\"Download Script Master CSV\")\n",
        "                status_box_master = gr.Textbox(label=\"Status\", interactive=False)\n",
        "\n",
        "            download_scrip_master_btn.click(\n",
        "                download_script_master_csv,\n",
        "                inputs=exchange_master,\n",
        "                outputs=[scrip_master_file, status_box_master])\n",
        "\n",
        "        with gr.TabItem(\"Portfolio\"):\n",
        "            gr.Markdown(\"## View Your Portfolio\")\n",
        "            portfolio_output = gr.DataFrame(label=\"Portfolio Holdings\", interactive=False)\n",
        "            download_btn = gr.File(label=\"Download Portfolio CSV\")\n",
        "\n",
        "            get_portfolio_btn = gr.Button(\"Get Portfolio Holdings\")\n",
        "            get_portfolio_btn.click(\n",
        "                get_portfolio_holdings,\n",
        "                outputs=[portfolio_output, download_btn]\n",
        "            )\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        with gr.TabItem(\"Watchlist Manager\"):\n",
        "          gr.Markdown(\"# ðŸ“ˆ Watchlist Manager\")\n",
        "\n",
        "          df_master_state = gr.State()\n",
        "\n",
        "          with gr.Row():\n",
        "            symbol_dropdown = gr.Dropdown(label=\"Select a Company to View/Edit\", scale=3)\n",
        "            generate_btn = gr.Button(\"ðŸ¤– Generate AI Remark\", variant=\"secondary\", scale=1)\n",
        "            save_btn = gr.Button(\"ðŸ’¾ Save All Changes\", variant=\"primary\", scale=1)\n",
        "\n",
        "          status_label = gr.Label(show_label=False)\n",
        "\n",
        "          gr.Markdown(\"---\")\n",
        "          gr.Markdown(\"### âœï¸ Edit Selected Company Details\")\n",
        "          symbol_detail_view = gr.DataFrame(interactive=True, wrap=False)\n",
        "\n",
        "          gr.Markdown(\"---\")\n",
        "          gr.Markdown(\"### â­ Your Watchlist\")\n",
        "          with gr.Row():\n",
        "            save_watchlist_btn = gr.Button(\"ðŸ’¾ Save Watchlist Changes\", variant=\"primary\")\n",
        "            load_watchlist_btn = gr.Button(\"ðŸ”„ Load Watchlist\", variant=\"secondary\")\n",
        "          watchlist_df_view = gr.DataFrame(interactive=True,wrap=False)\n",
        "\n",
        "          iface.load(\n",
        "              fn=get_initial_views,\n",
        "              outputs=[df_master_state, symbol_dropdown, symbol_detail_view, watchlist_df_view]\n",
        "          )\n",
        "          symbol_dropdown.change(\n",
        "              fn=display_selected_symbol,\n",
        "              inputs=[df_master_state, symbol_dropdown],\n",
        "              outputs=[symbol_detail_view]\n",
        "          )\n",
        "\n",
        "          symbol_detail_view.input(\n",
        "              fn=update_data_on_edit,\n",
        "              inputs=[df_master_state, symbol_dropdown, symbol_detail_view],\n",
        "              outputs=[df_master_state, watchlist_df_view]\n",
        "          )\n",
        "\n",
        "          generate_btn.click(\n",
        "              fn=generate_ai_remark,\n",
        "              inputs=[df_master_state, symbol_dropdown],\n",
        "              outputs=[df_master_state, symbol_detail_view, watchlist_df_view]\n",
        "          )\n",
        "          save_btn.click(\n",
        "              fn=save_data_to_csv,\n",
        "              inputs=[df_master_state],\n",
        "              outputs=[status_label, watchlist_df_view]\n",
        "          )\n",
        "\n",
        "          load_watchlist_btn.click(\n",
        "              fn=reload_watchlist,\n",
        "              inputs=[df_master_state],\n",
        "              outputs=[watchlist_df_view]\n",
        "          )\n",
        "\n",
        "          save_watchlist_btn.click(\n",
        "              fn=save_watchlist_changes,\n",
        "              inputs=[watchlist_df_view],\n",
        "              outputs=[status_label, watchlist_df_view]\n",
        "          )\n",
        "    gr.Markdown(\"## SWEET Pathway : Where smart trading meets SWEET success.\")\n",
        "    gr.Markdown(\"Conceptualized & Developed by Rohit Jain & Tanmay Jain\")\n",
        "\n",
        "\n",
        "    # --- 7. Event Handlers ---\n",
        "\n",
        "    # Generic UI helpers\n",
        "    def toggle_price_visibility(order_type):\n",
        "        return gr.update(visible=order_type == \"LIMIT\")\n",
        "\n",
        "    def update_symbol_dropdown(exchange):\n",
        "        symbol_list = nse_symbols_list if exchange == \"NSE\" else bse_symbols_list\n",
        "        return gr.update(choices=symbol_list)\n",
        "\n",
        "    buy_order_type.change(fn=toggle_price_visibility, inputs=buy_order_type, outputs=buy_price)\n",
        "    sell_order_type.change(fn=toggle_price_visibility, inputs=sell_order_type, outputs=sell_price)\n",
        "    buy_exchange.change(fn=update_symbol_dropdown, inputs=buy_exchange, outputs=buy_symbol)\n",
        "    sell_exchange.change(fn=update_symbol_dropdown, inputs=sell_exchange, outputs=sell_symbol)\n",
        "\n",
        "    # Manual Trading\n",
        "    place_buy_order_btn.click(fn=place_order, inputs=[buy_symbol, gr.Textbox(\"BUY\", visible=False), buy_qty, buy_order_type, buy_price, buy_exchange, buy_product], outputs=[manual_trade_status])\n",
        "    place_sell_order_btn.click(fn=place_order, inputs=[sell_symbol, gr.Textbox(\"SELL\", visible=False), sell_qty, sell_order_type, sell_price, sell_exchange, sell_product], outputs=[manual_trade_status])\n",
        "\n",
        "    # Use the new function to load the portfolio with checkboxes\n",
        "    load_mp_btn.click(fn=load_and_display_portfolio, outputs=[portfolio_df_editor, status_mp])\n",
        "\n",
        "    # Save button is for manual cell edits\n",
        "    save_mp_btn.click(fn=save_portfolio_to_csv, inputs=[portfolio_df_editor], outputs=[status_mp])\n",
        "\n",
        "    # Add row function\n",
        "    def add_row_to_df(df):\n",
        "        # Create a new row that matches the existing columns, including 'Delete'\n",
        "        new_row_data = {col: '' for col in df.columns}\n",
        "        if 'Delete' in new_row_data:\n",
        "            new_row_data['Delete'] = False # Default new rows to not be deleted\n",
        "        new_row_df = pd.DataFrame([new_row_data])\n",
        "        return pd.concat([df, new_row_df], ignore_index=True)\n",
        "    add_row_btn.click(fn=add_row_to_df, inputs=portfolio_df_editor, outputs=portfolio_df_editor)\n",
        "\n",
        "    # Connect the new delete button to the new delete_and_save_rows function\n",
        "    delete_row_btn.click(\n",
        "        fn=delete_and_save_rows,\n",
        "        inputs=[portfolio_df_editor],\n",
        "        outputs=[portfolio_df_editor, status_mp]\n",
        "    )\n",
        "\n",
        "    # Other Dashboard Handlers\n",
        "    mismatch_check_btn.click(fn=check_mismatches, outputs=[mismatch_output])\n",
        "    view_archive_btn.click(lambda: load_archive_from_csv(ARCHIVE_FILE), outputs=[archive_df_display])\n",
        "    preview_trades_btn.click(fn=preview_trades_from_tradebook, outputs=[trade_preview_df, status_mp])\n",
        "    confirm_add_btn.click(fn=add_preview_to_portfolio, inputs=[portfolio_df_editor, trade_preview_df], outputs=[portfolio_df_editor, status_mp])\n",
        "\n",
        "    # Automated Trading Handlers\n",
        "    analyze_suggest_btn.click(fn=on_analyze_and_suggest_click, outputs=[auto_trade_status, suggestions_df_output, portfolio_df_editor])\n",
        "    execute_trades_btn.click(fn=execute_selected_trades, inputs=[suggestions_df_output], outputs=[execution_status])\n",
        "\n",
        "    archive_button.click(fn=run_archiving_process,inputs=None)\n",
        "\n"
      ],
      "metadata": {
        "id": "zys8ffTAaWrE"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    iface.launch(debug = False, inline= False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DWs--6ZgaahM",
        "outputId": "d5388847-fa4a-4c35-d253-9bbc9fa88b35"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It looks like you are running Gradio on a hosted Jupyter notebook, which requires `share=True`. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://9c4ea0fb13b5319ef0.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MT5XS-otbBgi"
      },
      "execution_count": 60,
      "outputs": []
    }
  ]
}